import pandas as pd
from pathlib import Path
import chardet
from colorama import Fore, Style, init
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Configuration globale
SLOPE_PERIODS = 5
MAX_WORKERS = 4  # Parall√©lisation
# Initialiser colorama pour Windows
init(autoreset=True)
from Tools.func_features_preprocessing import *

# Configuration
DIRECTION = "LONG"  # ou "SHORT" selon votre besoin

DIR = (r"C:\\Users\\aulac\\OneDrive\\Documents\\Trading\\VisualStudioProject"
       r"\\Sierra_chart\\xTickReversal\\simu\\5_0_5TP_6SL_samedi2706\\merge")
TEMPLATE = (DIR +
            rf"\\Step5_5_0_5TP_6SL_010124_200625_extractOnlyFullSession_Only{DIRECTION}_feat__split{{split}}.csv")

CSV_TRAIN = TEMPLATE.format(split="1_01012024_01052024")
CSV_TEST = TEMPLATE.format(split="2_01052024_30092024")
CSV_VAL = TEMPLATE.format(split="3_30092024_28022025")
CSV_VAL1 = TEMPLATE.format(split="4_02032025_15052025")
CSV_UNSEEN = TEMPLATE.format(split="5_15052025_20062025")


def detect_file_encoding(file_path: str) -> str:
    """
    D√©tecte l'encodage d'un fichier
    """
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read(100000)  # Lire les premiers 100KB
            result = chardet.detect(raw_data)
            return result['encoding'] or 'utf-8'
    except Exception as e:
        print(f"‚ùå Erreur d√©tection encodage: {e}")
        return 'utf-8'


def load_csv_complete(path: str | Path) -> tuple[pd.DataFrame, pd.DataFrame, int]:
    """
    Charge les donn√©es compl√®tes ET filtr√©es s√©par√©ment
    Returns:
        df_complete: DataFrame avec TOUTES les bougies chronologiques (pour Rogers-Satchell)
        df_filtered: DataFrame avec seulement class_binaire ‚àà {0, 1} (pour les m√©triques)
        nb_sessions: Nombre de sessions
    """
    path = Path(path)

    if not path.exists():
        print(f"‚ùå Fichier introuvable: {path}")
        return pd.DataFrame(), pd.DataFrame(), 0

    encoding = detect_file_encoding(str(path))
    if encoding.lower() == "ascii":
        encoding = "ISO-8859-1"

    print(f"{path.name} ‚ûú encodage d√©tect√©: {encoding}")

    try:
        # Chargement COMPLET sans filtrage
        df_complete = pd.read_csv(path, sep=";", encoding=encoding, parse_dates=["date"], low_memory=False)
    except Exception as e:
        print(f"‚ùå Erreur chargement {path.name}: {e}")
        return pd.DataFrame(), pd.DataFrame(), 0

    # Correction de sc_sessionStartEnd
    df_complete["sc_sessionStartEnd"] = pd.to_numeric(df_complete["sc_sessionStartEnd"], errors="coerce")
    df_complete = df_complete.dropna(subset=["sc_sessionStartEnd"])
    df_complete["sc_sessionStartEnd"] = df_complete["sc_sessionStartEnd"].astype(int)

    # Compter les sessions
    nb_start = (df_complete["sc_sessionStartEnd"] == 10).sum()
    nb_end = (df_complete["sc_sessionStartEnd"] == 20).sum()
    nb_sessions = min(nb_start, nb_end)

    if nb_start != nb_end:
        print(f"{Fore.YELLOW}‚ö†Ô∏è Incoh√©rence sessions: {nb_start} d√©buts vs {nb_end} fins{Style.RESET_ALL}")
    else:
        print(f"{Fore.GREEN}‚úî {nb_sessions} sessions compl√®tes d√©tect√©es{Style.RESET_ALL}")

    # Num√©rotation des sessions
    df_complete["session_id"] = (df_complete["sc_sessionStartEnd"] == 10).cumsum().astype("int32")

    # ‚úÖ CORRECTION DU BUG: Cr√©er le dataset FILTR√â SANS reset_index
    df_filtered = df_complete[df_complete["class_binaire"].isin([0, 1])].copy()
    # ‚ùå LIGNE SUPPRIM√âE: df_filtered.reset_index(drop=True, inplace=True)

    print(f"üìä Donn√©es compl√®tes: {len(df_complete):,} bougies")
    print(f"üìä Donn√©es filtr√©es: {len(df_filtered):,} bougies ({len(df_filtered) / len(df_complete):.1%})")

    return df_complete, df_filtered, nb_sessions


def analyze_winrate_performance(df_filtered, name):
    """
    Analyse les performances brutes AVANT filtrage par algorithme
    """
    if df_filtered.empty:
        print(f"‚ùå {name}: DataFrame vide")
        return None

    # Distribution des classes
    class_counts = df_filtered['class_binaire'].value_counts().sort_index()
    total_trades = len(df_filtered)

    # Calculs de base
    wins = class_counts.get(1, 0)  # Trades r√©ussis
    losses = class_counts.get(0, 0)  # Trades √©chou√©s
    winrate = (wins / total_trades * 100) if total_trades > 0 else 0

    # Analyse par session
    session_stats = df_filtered.groupby('session_id').agg({
        'class_binaire': ['count', 'sum', 'mean']
    }).round(3)
    session_stats.columns = ['trades_per_session', 'wins_per_session', 'winrate_per_session']

    # M√©triques avanc√©es par session
    session_winrates = session_stats['winrate_per_session'] * 100
    session_trade_counts = session_stats['trades_per_session']

    stats = {
        'name': name,
        'total_trades': total_trades,
        'wins': wins,
        'losses': losses,
        'winrate': winrate,
        'trades_per_session_mean': session_trade_counts.mean(),
        'trades_per_session_std': session_trade_counts.std(),
        'trades_per_session_median': session_trade_counts.median(),
        'winrate_per_session_mean': session_winrates.mean(),
        'winrate_per_session_std': session_winrates.std(),
        'winrate_per_session_median': session_winrates.median(),
        'best_session_winrate': session_winrates.max(),
        'worst_session_winrate': session_winrates.min(),
        'sessions_count': len(session_stats),
        'profitable_sessions': (session_winrates > 50).sum(),
        'profitable_sessions_pct': (session_winrates > 50).mean() * 100
    }

    return stats, session_stats


def analyze_temporal_patterns(df_filtered, name):
    """
    Analyse les patterns temporels qui pourraient expliquer les diff√©rences de performance
    """
    if df_filtered.empty or 'date' not in df_filtered.columns:
        return None

    # Extraire les composantes temporelles
    df_temp = df_filtered.copy()
    df_temp['hour'] = df_temp['date'].dt.hour
    df_temp['day_of_week'] = df_temp['date'].dt.dayofweek  # 0=Lundi, 6=Dimanche
    df_temp['month'] = df_temp['date'].dt.month

    # Analyse par heure
    hourly_stats = df_temp.groupby('hour').agg({
        'class_binaire': ['count', 'mean']
    }).round(3)
    hourly_stats.columns = ['trades_count', 'winrate']
    hourly_stats['winrate'] *= 100

    # Analyse par jour de la semaine
    daily_stats = df_temp.groupby('day_of_week').agg({
        'class_binaire': ['count', 'mean']
    }).round(3)
    daily_stats.columns = ['trades_count', 'winrate']
    daily_stats['winrate'] *= 100

    # Analyse par mois
    monthly_stats = df_temp.groupby('month').agg({
        'class_binaire': ['count', 'mean']
    }).round(3)
    monthly_stats.columns = ['trades_count', 'winrate']
    monthly_stats['winrate'] *= 100

    return {
        'hourly': hourly_stats,
        'daily': daily_stats,
        'monthly': monthly_stats
    }


def analyze_market_conditions(df_filtered, name):
    """
    Analyse les conditions de march√© qui pourraient affecter les performances
    """
    if df_filtered.empty:
        return None

    # Colonnes d'int√©r√™t pour l'analyse des conditions de march√©
    market_cols = []
    for col in df_filtered.columns:
        if any(keyword in col.lower() for keyword in ['volume', 'volatility', 'spread', 'duration', 'atr']):
            market_cols.append(col)

    if not market_cols:
        return None

    # Diviser les trades en gagnants/perdants
    winners = df_filtered[df_filtered['class_binaire'] == 1]
    losers = df_filtered[df_filtered['class_binaire'] == 0]

    comparisons = {}
    for col in market_cols:
        if col in df_filtered.columns:
            try:
                win_mean = winners[col].mean()
                loss_mean = losers[col].mean()
                overall_mean = df_filtered[col].mean()

                comparisons[col] = {
                    'winners_mean': win_mean,
                    'losers_mean': loss_mean,
                    'overall_mean': overall_mean,
                    'difference': win_mean - loss_mean,
                    'difference_pct': ((win_mean - loss_mean) / overall_mean * 100) if overall_mean != 0 else 0
                }
            except:
                continue

    return comparisons


def analyze_market_regimes(datasets_dict, dataset_names):
    """
    Analyse approfondie des diff√©rences de r√©gimes de march√© entre datasets
    Focus sur les indicateurs qui caract√©risent TEST vs TOUS les autres datasets
    """

    print("\n" + "=" * 80)
    print("üîç ANALYSE DES R√âGIMES DE MARCH√â - DIAGNOSTIC TEST vs TOUS")
    print("=" * 80)

    # ====== 1. ANALYSE TEMPORELLE DES BOUGIES ======
    print("\nüìä 1. ANALYSE TEMPORELLE DES BOUGIES")
    print("-" * 50)

    candle_stats = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_complete.empty:
            continue

        # Statistiques des dur√©es de bougies
        durations = df_complete['sc_candleDuration'].dropna()
        volumes = df_complete['sc_volume_perTick'].dropna()

        stats_dict = {
            'duration_mean': durations.mean(),
            'duration_median': durations.median(),
            'duration_std': durations.std(),
            'duration_cv': durations.std() / durations.mean() * 100,
            'duration_skew': stats.skew(durations),
            'duration_kurt': stats.kurtosis(durations),
            'volume_mean': volumes.mean(),
            'volume_median': volumes.median(),
            'volume_std': volumes.std(),
            'volume_cv': volumes.std() / volumes.mean() * 100,
            'duration_p95': durations.quantile(0.95),
            'duration_p05': durations.quantile(0.05),
            'fast_candles_pct': (durations < 10).mean() * 100,  # % bougies < 10s
            'slow_candles_pct': (durations > 300).mean() * 100,  # % bougies > 5min
        }

        candle_stats[name] = stats_dict

    # Affichage comparatif
    df_stats = pd.DataFrame(candle_stats).T

    print("DUR√âE MOYENNE DES BOUGIES:")
    for name in dataset_names:
        if name in df_stats.index:
            print(
                f"{name:12}: {df_stats.loc[name, 'duration_mean']:6.1f}s (CV: {df_stats.loc[name, 'duration_cv']:5.1f}%)")

    print("\nVOLUME MOYEN PAR TICK:")
    for name in dataset_names:
        if name in df_stats.index:
            print(f"{name:12}: {df_stats.loc[name, 'volume_mean']:6.1f} (CV: {df_stats.loc[name, 'volume_cv']:5.1f}%)")

    print("\nDISTRIBUTION DES VITESSES:")
    for name in dataset_names:
        if name in df_stats.index:
            print(
                f"{name:12}: {df_stats.loc[name, 'fast_candles_pct']:4.1f}% rapides (<10s) | {df_stats.loc[name, 'slow_candles_pct']:4.1f}% lentes (>5min)")

    # ====== 2. ANALYSE DES PATTERNS VPT ======
    print("\nüìà 2. ANALYSE DES INDICATEURS VPT")
    print("-" * 50)

    vpt_analysis = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_filtered.empty:
            continue

        # Chercher toutes les colonnes VPT
        vpt_cols = [col for col in df_filtered.columns if 'vpt' in col.lower()]

        if not vpt_cols:
            continue

        winners = df_filtered[df_filtered['class_binaire'] == 1]
        losers = df_filtered[df_filtered['class_binaire'] == 0]

        vpt_diffs = {}
        for col in vpt_cols:
            if col in df_filtered.columns:
                win_mean = winners[col].mean()
                loss_mean = losers[col].mean()
                diff = win_mean - loss_mean
                diff_pct = (diff / df_filtered[col].mean() * 100) if df_filtered[col].mean() != 0 else 0

                vpt_diffs[col] = {
                    'difference': diff,
                    'difference_pct': diff_pct,
                    'overall_mean': df_filtered[col].mean()
                }

        vpt_analysis[name] = vpt_diffs

    # Identifier les VPT avec des comportements oppos√©s - TEST vs TOUS
    divergent_signals = []

    if 'TEST' in vpt_analysis:
        test_vpt = vpt_analysis['TEST']

        # Calculer moyennes des autres datasets
        other_datasets = {k: v for k, v in vpt_analysis.items() if k != 'TEST'}

        for vpt_name in test_vpt.keys():
            # Calculer moyenne des effets pour tous les autres datasets
            other_effects = []
            for other_name, other_vpt in other_datasets.items():
                if vpt_name in other_vpt:
                    other_effects.append(other_vpt[vpt_name]['difference_pct'])

            if other_effects:
                test_effect = test_vpt[vpt_name]['difference_pct']
                others_avg = np.mean(other_effects)
                divergence = abs(test_effect - others_avg)

                # Signaux divergents si |√©cart| > 15%
                if divergence > 15:
                    divergent_signals.append({
                        'vpt': vpt_name,
                        'test_effect': test_effect,
                        'others_avg': others_avg,
                        'divergence_strength': divergence,
                        'is_opposite': (test_effect * others_avg < 0)
                    })

        # Trier par force de divergence
        divergent_signals.sort(key=lambda x: x['divergence_strength'], reverse=True)

        print(f"üö® {len(divergent_signals)} VPT avec COMPORTEMENTS DIVERGENTS d√©tect√©s:")
        for signal in divergent_signals[:8]:  # Top 8
            opposite_flag = " [OPPOS√â]" if signal['is_opposite'] else ""
            print(f"   {signal['vpt'][:35]:35}{opposite_flag}")
            print(f"      TEST:   {signal['test_effect']:+6.1f}% | AUTRES: {signal['others_avg']:+6.1f}%")
            print(f"      √âcart:  {signal['divergence_strength']:5.1f}%")

    # ====== 3. ANALYSE DES CONDITIONS TEMPORELLES ======
    print("\n‚è∞ 3. ANALYSE DES CONDITIONS TEMPORELLES")
    print("-" * 50)

    temporal_analysis = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_filtered.empty or 'date' not in df_filtered.columns:
            continue

        df_temp = df_filtered.copy()
        df_temp['hour'] = df_temp['date'].dt.hour
        df_temp['day_of_week'] = df_temp['date'].dt.dayofweek

        # Analyser la distribution temporelle des trades
        hourly_dist = df_temp['hour'].value_counts().sort_index()
        daily_dist = df_temp['day_of_week'].value_counts().sort_index()

        # Analyser les performances par heure/jour
        hourly_perf = df_temp.groupby('hour')['class_binaire'].agg(['count', 'mean'])
        daily_perf = df_temp.groupby('day_of_week')['class_binaire'].agg(['count', 'mean'])

        temporal_analysis[name] = {
            'hourly_dist': hourly_dist,
            'daily_dist': daily_dist,
            'hourly_perf': hourly_perf,
            'daily_perf': daily_perf,
            'peak_trading_hour': hourly_dist.idxmax(),
            'peak_trading_day': daily_dist.idxmax(),
            'best_hour': hourly_perf['mean'].idxmax(),
            'worst_hour': hourly_perf['mean'].idxmin(),
            'trading_concentration': (hourly_dist.max() / hourly_dist.sum()) * 100
        }

    print("CONCENTRATION DU TRADING:")
    for name in dataset_names:
        if name in temporal_analysis:
            analysis = temporal_analysis[name]
            print(
                f"{name:12}: Pic √† {analysis['peak_trading_hour']:2d}h ({analysis['trading_concentration']:4.1f}% du volume)")

    # Analyse sp√©cifique TEST vs AUTRES
    if 'TEST' in temporal_analysis:
        test_temp = temporal_analysis['TEST']

        print(f"\nüéØ SP√âCIFICIT√âS TEMPORELLES DE TEST:")
        print(f"   Heure de pic:     {test_temp['peak_trading_hour']}h")
        print(f"   Meilleure heure:  {test_temp['best_hour']}h")
        print(f"   Pire heure:       {test_temp['worst_hour']}h")
        print(f"   Concentration:    {test_temp['trading_concentration']:.1f}%")

        # Comparer avec les autres
        other_concentrations = []
        for name, analysis in temporal_analysis.items():
            if name != 'TEST':
                other_concentrations.append(analysis['trading_concentration'])

        if other_concentrations:
            avg_concentration = np.mean(other_concentrations)
            concentration_diff = test_temp['trading_concentration'] - avg_concentration

            print(f"\n   Comparaison concentration:")
            print(f"   TEST: {test_temp['trading_concentration']:.1f}% vs AUTRES: {avg_concentration:.1f}%")
            if abs(concentration_diff) > 5:
                status = "plus concentr√©" if concentration_diff > 0 else "plus dispers√©"
                print(f"   ‚Üí TEST est {status} temporellement ({concentration_diff:+.1f}%)")

    # ====== 4. R√âGIMES DE VOLATILIT√â ======
    print("\nüå™Ô∏è  4. ANALYSE DES R√âGIMES DE VOLATILIT√â")
    print("-" * 50)

    volatility_analysis = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_complete.empty:
            continue

        # Chercher colonnes de volatilit√©
        vol_cols = []
        for col in df_complete.columns:
            if any(keyword in col.lower() for keyword in ['atr', 'volatility', 'range', 'spread']):
                vol_cols.append(col)

        if vol_cols:
            # Calculer moyennes de volatilit√©
            vol_means = {}
            for col in vol_cols:
                vol_means[col] = df_complete[col].mean()

            volatility_analysis[name] = vol_means

    if volatility_analysis:
        print("NIVEAUX DE VOLATILIT√â MOYENS:")
        vol_cols_found = set()
        for analysis in volatility_analysis.values():
            vol_cols_found.update(analysis.keys())

        for col in list(vol_cols_found)[:5]:  # Top 5 indicateurs
            print(f"\n{col[:30]:30}:")
            for name in dataset_names:
                if name in volatility_analysis and col in volatility_analysis[name]:
                    print(f"   {name:12}: {volatility_analysis[name][col]:8.3f}")

        # Analyse sp√©cifique TEST vs AUTRES
        if 'TEST' in volatility_analysis:
            test_vol = volatility_analysis['TEST']

            print(f"\nüéØ COMPARAISON VOLATILIT√â TEST vs AUTRES:")

            for col in list(vol_cols_found)[:3]:  # Top 3 indicateurs
                if col in test_vol:
                    test_value = test_vol[col]
                    other_values = []

                    for name, vol_data in volatility_analysis.items():
                        if name != 'TEST' and col in vol_data:
                            other_values.append(vol_data[col])

                    if other_values:
                        others_avg = np.mean(other_values)
                        diff_pct = ((test_value - others_avg) / others_avg * 100) if others_avg != 0 else 0

                        status = "üî¥" if abs(diff_pct) > 20 else "üü°" if abs(diff_pct) > 10 else "üü¢"
                        direction = "plus √©lev√©" if diff_pct > 0 else "plus bas"

                        print(f"   {status} {col[:25]:25}: TEST {direction} de {abs(diff_pct):4.1f}%")

    # ====== 5. RANKING ANALYSE : TEST vs ALL ======
    print("\nüèÜ 5. POSITIONNEMENT DE TEST vs TOUS LES DATASETS")
    print("-" * 60)

    quality_percentile = None

    if 'TEST' in df_stats.index and len(df_stats) > 1:
        test_stats = df_stats.loc['TEST']

        print("CLASSEMENTS DE TEST (1=meilleur selon contexte, 5=pire):")

        # Dur√©e moyenne (rang selon vitesse - plus court peut √™tre mieux ou pire selon strat√©gie)
        duration_rank = (df_stats['duration_mean'] < test_stats['duration_mean']).sum() + 1
        duration_percentile = (len(df_stats) - duration_rank + 1) / len(df_stats) * 100
        print(f"‚è±Ô∏è  Vitesse bougies     : {duration_rank}/{len(df_stats)} (Percentile: {duration_percentile:.0f}%)")

        # Volume par tick (plus √©lev√© g√©n√©ralement mieux)
        volume_rank = (df_stats['volume_mean'] > test_stats['volume_mean']).sum() + 1
        volume_percentile = (len(df_stats) - volume_rank + 1) / len(df_stats) * 100
        print(f"üìä Volume/tick         : {volume_rank}/{len(df_stats)} (Percentile: {volume_percentile:.0f}%)")

        # Stabilit√© dur√©e (CV plus bas = plus stable = mieux)
        cv_duration_rank = (df_stats['duration_cv'] < test_stats['duration_cv']).sum() + 1
        cv_duration_percentile = (len(df_stats) - cv_duration_rank + 1) / len(df_stats) * 100
        print(f"üéØ Stabilit√© dur√©e     : {cv_duration_rank}/{len(df_stats)} (Percentile: {cv_duration_percentile:.0f}%)")

        # Stabilit√© volume (CV plus bas = plus stable = mieux)
        cv_volume_rank = (df_stats['volume_cv'] < test_stats['volume_cv']).sum() + 1
        cv_volume_percentile = (len(df_stats) - cv_volume_rank + 1) / len(df_stats) * 100
        print(f"üìà Stabilit√© volume    : {cv_volume_rank}/{len(df_stats)} (Percentile: {cv_volume_percentile:.0f}%)")

        # Bougies rapides (selon strat√©gie, peut √™tre bon ou mauvais)
        fast_rank = (df_stats['fast_candles_pct'] < test_stats['fast_candles_pct']).sum() + 1
        fast_percentile = (len(df_stats) - fast_rank + 1) / len(df_stats) * 100
        print(f"‚ö° Bougies rapides     : {fast_rank}/{len(df_stats)} (Percentile: {fast_percentile:.0f}%)")

        # Score global de "qualit√©" des conditions de march√©
        # Plus le rang est bas, meilleures sont les conditions (stabilit√© √©lev√©e = bon)
        quality_score = cv_duration_rank + cv_volume_rank
        max_quality_score = 2 * len(df_stats)
        quality_percentile = (max_quality_score - quality_score + 2) / max_quality_score * 100

        print(f"\nüéØ SCORE QUALIT√â CONDITIONS: {quality_percentile:.0f}% (stabilit√© g√©n√©rale)")

        if quality_percentile >= 80:
            print("   ‚úÖ TEST = Conditions de march√© EXCELLENTES")
        elif quality_percentile >= 60:
            print("   üü¢ TEST = Conditions de march√© BONNES")
        elif quality_percentile >= 40:
            print("   üü° TEST = Conditions de march√© MOYENNES")
        elif quality_percentile >= 20:
            print("   üü† TEST = Conditions de march√© DIFFICILES")
        else:
            print("   üî¥ TEST = Conditions de march√© TR√àS DIFFICILES")

        # Analyse d√©taill√©e des √©carts
        print(f"\nüìä √âCARTS SIGNIFICATIFS DE TEST:")

        # Dur√©e
        duration_diff_pct = (test_stats['duration_mean'] - df_stats['duration_mean'].mean()) / df_stats[
            'duration_mean'].mean() * 100
        if abs(duration_diff_pct) > 15:
            direction = "plus rapides" if duration_diff_pct < 0 else "plus lentes"
            print(f"   ‚è±Ô∏è  Bougies {direction} de {abs(duration_diff_pct):.1f}% vs moyenne")

        # Volume
        volume_diff_pct = (test_stats['volume_mean'] - df_stats['volume_mean'].mean()) / df_stats[
            'volume_mean'].mean() * 100
        if abs(volume_diff_pct) > 15:
            direction = "plus √©lev√©" if volume_diff_pct > 0 else "plus bas"
            print(f"   üìä Volume {direction} de {abs(volume_diff_pct):.1f}% vs moyenne")

        # Stabilit√©
        cv_diff = test_stats['duration_cv'] - df_stats['duration_cv'].mean()
        if abs(cv_diff) > 100:  # CV en %
            direction = "moins stable" if cv_diff > 0 else "plus stable"
            print(f"   üéØ Dur√©es {direction} de {abs(cv_diff):.0f} points de CV vs moyenne")

    # ====== 6. RECOMMANDATIONS FINALES ======
    print("\nüí° RECOMMANDATIONS BAS√âES SUR L'ANALYSE TEST vs TOUS")
    print("=" * 70)

    recommendations = []
    priority_actions = []

    if 'TEST' in df_stats.index:
        # Bas√© sur la qualit√© des conditions
        if quality_percentile and quality_percentile < 40:
            recommendations.append("üéØ PRIORIT√â 1: Conditions de march√© difficiles d√©tect√©es pour TEST")
            priority_actions.append("   ‚Üí Adapter les param√®tres √† la volatilit√© √©lev√©e")
            priority_actions.append("   ‚Üí Consid√©rer des stops plus larges ou des TP plus courts")

        # Bas√© sur les VPT divergents
        if len(divergent_signals) > 5:
            recommendations.append("üéØ PRIORIT√â 2: Instabilit√© majeure des signaux VPT")
            priority_actions.append("   ‚Üí Re-calibrer TOUS les seuils VPT pour TEST")
            priority_actions.append("   ‚Üí Impl√©menter une d√©tection de r√©gime de march√©")
        elif len(divergent_signals) > 2:
            recommendations.append("üéØ PRIORIT√â 2: Quelques signaux VPT instables")
            priority_actions.append("   ‚Üí Re-v√©rifier les VPT les plus divergents")

        # Bas√© sur la temporalit√©
        if 'TEST' in temporal_analysis:
            test_conc = temporal_analysis['TEST']['trading_concentration']
            if test_conc > 15:
                recommendations.append("üéØ PRIORIT√â 4: Trading tr√®s concentr√© temporellement")
                priority_actions.append("   ‚Üí Optimiser sp√©cifiquement pour les heures de pic")

        # Recommandations g√©n√©rales
        if not recommendations:
            recommendations.append("‚úÖ Les diff√©rences TEST vs AUTRES sont dans la normale")
            priority_actions.append("   ‚Üí Optimisation fine des hyperparam√®tres suffisante")
            priority_actions.append("   ‚Üí Surveiller la stabilit√© sur donn√©es futures")

        print("ACTIONS RECOMMAND√âES:")
        for rec in recommendations:
            print(rec)

        for action in priority_actions:
            print(action)

    print(f"\nüî¨ INDICATEURS CL√âS √Ä SURVEILLER:")
    print(f"   1. Stabilit√© temporelle des VPT (actuellement {len(divergent_signals)} divergents)")
    print(f"   2. Niveau de volatilit√© relative vs autres p√©riodes")
    print(f"   3. Concentration temporelle des trades")
    print(f"   4. Corr√©lations features-performance")

    return {
        'candle_stats': df_stats,
        'vpt_analysis': vpt_analysis,
        'temporal_analysis': temporal_analysis,
        'volatility_analysis': volatility_analysis,
        'divergent_signals': divergent_signals,
        'quality_score': quality_percentile,
        'recommendations': recommendations
    }


@njit
def fast_percentile(arr, q):
    """Calcul rapide de percentile avec numba"""
    sorted_arr = np.sort(arr)
    n = len(sorted_arr)
    index = (n - 1) * q / 100.0
    lower = int(index)
    upper = lower + 1
    weight = index - lower

    if upper >= n:
        return sorted_arr[-1]
    return sorted_arr[lower] * (1 - weight) + sorted_arr[upper] * weight


@njit
def fast_stats_calculation(values):
    """Calcul ultra-rapide des statistiques de base avec numba"""
    if len(values) == 0:
        return np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])

    mean_val = np.mean(values)
    median_val = np.median(values)
    std_val = np.std(values)
    min_val = np.min(values)
    max_val = np.max(values)
    q25_val = fast_percentile(values, 25)
    q75_val = fast_percentile(values, 75)

    return np.array([mean_val, median_val, std_val, min_val, max_val, q25_val, q75_val])


def print_winrate_analysis(stats_dict):
    """
    Affiche l'analyse d√©taill√©e des winrates
    """
    print("\n" + "=" * 80)
    print("üéØ ANALYSE DES PERFORMANCES BRUTES (AVANT FILTRAGE ALGORITHME)")
    print("=" * 80)

    for name, (stats, session_stats) in stats_dict.items():
        if stats is None:
            continue

        print(f"\n=== {name} ===")
        print(f"üìä Total trades: {stats['total_trades']:,}")
        print(f"‚úÖ Wins: {stats['wins']:,} | ‚ùå Losses: {stats['losses']:,}")
        print(f"üéØ Winrate global: {stats['winrate']:.2f}%")

        print(f"\nüìà ANALYSE PAR SESSION:")
        print(f"üèüÔ∏è  Sessions analys√©es: {stats['sessions_count']}")
        print(
            f"üìä Trades/session - Moyenne: {stats['trades_per_session_mean']:.1f} | M√©diane: {stats['trades_per_session_median']:.1f} | √âcart-type: {stats['trades_per_session_std']:.1f}")
        print(
            f"üéØ Winrate/session - Moyenne: {stats['winrate_per_session_mean']:.2f}% | M√©diane: {stats['winrate_per_session_median']:.2f}% | √âcart-type: {stats['winrate_per_session_std']:.2f}%")
        print(
            f"üèÜ Meilleure session: {stats['best_session_winrate']:.2f}% | üíÄ Pire session: {stats['worst_session_winrate']:.2f}%")
        print(
            f"üí∞ Sessions profitables (>50%): {stats['profitable_sessions']}/{stats['sessions_count']} ({stats['profitable_sessions_pct']:.1f}%)")


def print_comparative_winrate_analysis(stats_dict):
    """
    Analyse comparative des winrates entre datasets
    """
    print("\n" + "=" * 80)
    print("üîç ANALYSE COMPARATIVE DES WINRATES")
    print("=" * 80)

    # Extraire les stats valides
    valid_stats = {name: stats for name, (stats, _) in stats_dict.items() if stats is not None}

    if len(valid_stats) < 2:
        print("‚ùå Pas assez de datasets pour une analyse comparative")
        return

    # Classement par winrate
    sorted_by_winrate = sorted(valid_stats.items(), key=lambda x: x[1]['winrate'], reverse=True)

    print("üèÜ CLASSEMENT PAR WINRATE GLOBAL:")
    for i, (name, stats) in enumerate(sorted_by_winrate, 1):
        medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}."
        print(f"{medal} {name:12}: {stats['winrate']:6.2f}% ({stats['total_trades']:,} trades)")

    # Analyse des √©carts
    best_name, best_stats = sorted_by_winrate[0]
    worst_name, worst_stats = sorted_by_winrate[-1]

    print(f"\nüìä ANALYSE DES √âCARTS:")
    print(f"üèÜ Meilleur: {best_name} ({best_stats['winrate']:.2f}%)")
    print(f"üíÄ Pire: {worst_name} ({worst_stats['winrate']:.2f}%)")
    print(f"üìè √âcart: {best_stats['winrate'] - worst_stats['winrate']:.2f} points de %")

    # Analyse de la consistance (√©cart-type des winrates par session)
    print(f"\nüéØ ANALYSE DE LA CONSISTANCE (√âcart-type winrate/session):")
    sorted_by_consistency = sorted(valid_stats.items(), key=lambda x: x[1]['winrate_per_session_std'])

    for name, stats in sorted_by_consistency:
        consistency_score = "üü¢" if stats['winrate_per_session_std'] < 20 else "üü°" if stats[
                                                                                         'winrate_per_session_std'] < 30 else "üî¥"
        print(
            f"{consistency_score} {name:12}: {stats['winrate_per_session_std']:5.2f}% (plus c'est bas, plus c'est consistant)")

    # Focus sur TEST si il sous-performe
    if 'TEST' in valid_stats:
        test_stats = valid_stats['TEST']
        test_rank = next(i for i, (name, _) in enumerate(sorted_by_winrate, 1) if name == 'TEST')

        if test_rank > 2:  # Si TEST n'est pas dans le top 2
            print(f"\nüîç ANALYSE SP√âCIFIQUE - POURQUOI TEST SOUS-PERFORME:")
            print(f"üìâ Position: {test_rank}/{len(valid_stats)} avec {test_stats['winrate']:.2f}% de winrate")

            # Comparaison avec le meilleur
            if best_name != 'TEST':
                print(f"üìä Vs {best_name}:")
                print(f"   ‚Ä¢ √âcart winrate: -{best_stats['winrate'] - test_stats['winrate']:.2f} points")
                print(
                    f"   ‚Ä¢ Trades/session: TEST={test_stats['trades_per_session_mean']:.1f} vs {best_name}={best_stats['trades_per_session_mean']:.1f}")
                print(
                    f"   ‚Ä¢ Consistance: TEST={test_stats['winrate_per_session_std']:.2f}% vs {best_name}={best_stats['winrate_per_session_std']:.2f}%")
                print(
                    f"   ‚Ä¢ Sessions profitables: TEST={test_stats['profitable_sessions_pct']:.1f}% vs {best_name}={best_stats['profitable_sessions_pct']:.1f}%")


def print_temporal_analysis(temporal_dict):
    """
    Affiche l'analyse temporelle
    """
    print("\n" + "=" * 80)
    print("‚è∞ ANALYSE DES PATTERNS TEMPORELS")
    print("=" * 80)

    for name, patterns in temporal_dict.items():
        if patterns is None:
            continue

        print(f"\n=== {name} ===")

        # Analyse horaire
        if 'hourly' in patterns and not patterns['hourly'].empty:
            hourly = patterns['hourly']
            best_hour = hourly['winrate'].idxmax()
            worst_hour = hourly['winrate'].idxmin()

            print(f"üïê PERFORMANCE PAR HEURE:")
            print(
                f"üèÜ Meilleure heure: {best_hour}h ({hourly.loc[best_hour, 'winrate']:.2f}% - {hourly.loc[best_hour, 'trades_count']} trades)")
            print(
                f"üíÄ Pire heure: {worst_hour}h ({hourly.loc[worst_hour, 'winrate']:.2f}% - {hourly.loc[worst_hour, 'trades_count']} trades)")

            # Top 3 heures
            top_hours = hourly.nlargest(3, 'winrate')
            print(
                f"üìä Top 3 heures: {', '.join([f'{h}h({wr:.1f}%)' for h, wr in zip(top_hours.index, top_hours['winrate'])])}")

        # Analyse par jour de la semaine
        if 'daily' in patterns and not patterns['daily'].empty:
            daily = patterns['daily']
            days_names = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']

            print(f"\nüìÖ PERFORMANCE PAR JOUR:")
            for day_idx in daily.index:
                day_name = days_names[day_idx] if day_idx < len(days_names) else f'Jour{day_idx}'
                print(
                    f"   {day_name}: {daily.loc[day_idx, 'winrate']:5.2f}% ({daily.loc[day_idx, 'trades_count']} trades)")


def print_market_conditions_analysis(market_dict):
    """
    Affiche l'analyse des conditions de march√©
    """
    print("\n" + "=" * 80)
    print("üìà ANALYSE DES CONDITIONS DE MARCH√â")
    print("=" * 80)

    for name, conditions in market_dict.items():
        if conditions is None or not conditions:
            continue

        print(f"\n=== {name} ===")
        print("üîç Diff√©rences moyennes entre trades gagnants et perdants:")

        # Trier par importance de l'√©cart
        sorted_conditions = sorted(conditions.items(),
                                   key=lambda x: abs(x[1]['difference_pct']),
                                   reverse=True)

        for col, stats in sorted_conditions[:10]:  # Top 10 des diff√©rences
            direction = "üìà" if stats['difference'] > 0 else "üìâ"
            print(f"{direction} {col:25}: {stats['difference']:8.2f} ({stats['difference_pct']:+6.1f}%)")


def calculate_candle_stats(df_complete, df_name):
    """
    Calcule les statistiques de dur√©e des bougies et volume par tick pour un dataset
    """
    if df_complete.empty:
        print(f"‚ùå {df_name}: DataFrame vide")
        return None

    stats = {'count': len(df_complete)}

    # === ANALYSE DUR√âE DES BOUGIES ===
    if 'sc_candleDuration' in df_complete.columns:
        durations = df_complete['sc_candleDuration'].dropna()

        if len(durations) > 0:
            stats.update({
                'duration_count': len(durations),
                'duration_mean_seconds': durations.mean(),
                'duration_mean_minutes': durations.mean() / 60,
                'duration_median_seconds': durations.median(),
                'duration_std_seconds': durations.std(),
                'duration_min_seconds': durations.min(),
                'duration_max_seconds': durations.max(),
                'duration_q25': durations.quantile(0.25),
                'duration_q75': durations.quantile(0.75)
            })
        else:
            print(f"‚ö†Ô∏è  {df_name}: Aucune donn√©e de dur√©e valide")
    else:
        print(f"‚ö†Ô∏è  {df_name}: Colonne 'sc_candleDuration' introuvable")

    # === ANALYSE VOLUME PAR TICK ===
    if 'sc_volume_perTick' in df_complete.columns:
        volumes = df_complete['sc_volume_perTick'].dropna()

        if len(volumes) > 0:
            stats.update({
                'volume_count': len(volumes),
                'volume_mean': volumes.mean(),
                'volume_median': volumes.median(),
                'volume_std': volumes.std(),
                'volume_min': volumes.min(),
                'volume_max': volumes.max(),
                'volume_q25': volumes.quantile(0.25),
                'volume_q75': volumes.quantile(0.75)
            })
        else:
            print(f"‚ö†Ô∏è  {df_name}: Aucune donn√©e de volume par tick valide")
    else:
        print(f"‚ö†Ô∏è  {df_name}: Colonne 'sc_volume_perTick' introuvable")

    # === AFFICHAGE ===
    print(f"\n=== {df_name} ===")
    print(f"üìä Nombre total de bougies: {stats['count']:,}")

    # Dur√©e des bougies
    if 'duration_mean_seconds' in stats:
        print(f"\nüïê DUR√âE DES BOUGIES:")
        print(
            f"‚è±Ô∏è  Dur√©e moyenne: {stats['duration_mean_seconds']:.2f} secondes ({stats['duration_mean_minutes']:.2f} minutes)")
        print(f"üìà M√©diane: {stats['duration_median_seconds']:.2f}s")
        print(f"üìä √âcart-type: {stats['duration_std_seconds']:.2f}s")
        print(f"üìâ Min: {stats['duration_min_seconds']:.0f}s | Max: {stats['duration_max_seconds']:.0f}s")
        print(f"üì¶ Q25: {stats['duration_q25']:.1f}s | Q75: {stats['duration_q75']:.1f}s")

    # Volume par tick
    if 'volume_mean' in stats:
        print(f"\nüìà VOLUME PAR TICK:")
        print(f"üìä Volume moyen: {stats['volume_mean']:.2f}")
        print(f"üìà M√©diane: {stats['volume_median']:.2f}")
        print(f"üìä √âcart-type: {stats['volume_std']:.2f}")
        print(f"üìâ Min: {stats['volume_min']:.0f} | Max: {stats['volume_max']:.0f}")
        print(f"üì¶ Q25: {stats['volume_q25']:.1f} | Q75: {stats['volume_q75']:.1f}")

    return stats


def check_temporal_consistency(datasets):
    """
    V√©rifie si les dur√©es sont coh√©rentes avec les timestamps
    """
    print("\n" + "=" * 50)
    print("üïê V√âRIFICATION COH√âRENCE TEMPORELLE")
    print("=" * 50)

    for df, name in datasets:
        if df.empty:
            print(f"{name:12}: DataFrame vide - analyse impossible")
            continue

        if 'date' in df.columns and len(df) > 1:
            try:
                # Calculer les diff√©rences r√©elles entre timestamps
                df_sorted = df.sort_values('date').copy()
                time_diffs = df_sorted['date'].diff().dt.total_seconds().dropna()

                if len(time_diffs) > 0:
                    actual_mean = time_diffs.mean()
                    if 'sc_candleDuration' in df.columns:
                        reported_mean = df['sc_candleDuration'].mean()
                        if pd.notna(reported_mean) and reported_mean > 0:
                            diff_pct = abs(actual_mean - reported_mean) / actual_mean * 100

                            status = "‚úÖ" if diff_pct <= 10 else "‚ö†Ô∏è"
                            print(
                                f"{status} {name:12}: R√©el={actual_mean:.1f}s | Rapport√©={reported_mean:.1f}s | √âcart={diff_pct:.1f}%")

                            if diff_pct > 10:
                                print(f"   üîç {name}: √âcart significatif d√©tect√©!")
                        else:
                            print(f"‚ùå {name:12}: Dur√©es rapport√©es invalides")
                    else:
                        print(f"‚ùå {name:12}: Colonne sc_candleDuration manquante")
                else:
                    print(f"‚ùå {name:12}: Impossible de calculer les diff√©rences temporelles")
            except Exception as e:
                print(f"‚ùå {name:12}: Erreur analyse temporelle - {e}")
        else:
            print(f"‚ùå {name:12}: Colonne 'date' manquante ou donn√©es insuffisantes")


def analyze_pre_trade_conditions(datasets_dict, n_candles_before=10):
    """
    Analyse les conditions des N bougies pr√©c√©dant chaque trade
    pour identifier les diff√©rences entre datasets
    """

    print(f"\nüîç ANALYSE DES {n_candles_before} BOUGIES PR√âC√âDANT CHAQUE TRADE")
    print("=" * 80)

    results = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_complete.empty or df_filtered.empty:
            continue

        print(f"\nüìä DATASET: {name}")
        print("-" * 50)

        # Pr√©parer les donn√©es
        df_complete_sorted = df_complete.sort_values(['session_id', 'date']).reset_index(drop=True)

        # Identifier les indices des trades dans df_complete
        trade_indices = []
        for _, trade_row in df_filtered.iterrows():
            # Trouver l'indice correspondant dans df_complete
            matching_indices = df_complete_sorted[
                (df_complete_sorted['date'] == trade_row['date']) &
                (df_complete_sorted['session_id'] == trade_row['session_id'])
                ].index

            if len(matching_indices) > 0:
                trade_indices.append(matching_indices[0])

        print(f"Trades identifi√©s: {len(trade_indices)}")

        # Analyser les conditions pr√©-trade
        pre_trade_conditions = []

        for trade_idx in trade_indices:
            # V√©rifier qu'on a assez de bougies avant
            if trade_idx >= n_candles_before:
                # Extraire les N bougies pr√©c√©dentes
                start_idx = trade_idx - n_candles_before
                pre_candles = df_complete_sorted.iloc[start_idx:trade_idx]

                # Calculer les m√©triques
                conditions = calculate_pre_trade_metrics(pre_candles, n_candles_before)
                if conditions:
                    pre_trade_conditions.append(conditions)

        if pre_trade_conditions:
            # Agr√©ger les r√©sultats
            aggregated = aggregate_pre_trade_conditions(pre_trade_conditions, name)
            results[name] = aggregated

            # Afficher les r√©sultats
            display_pre_trade_analysis(aggregated, name)

    # Analyse comparative
    if len(results) > 1:
        compare_pre_trade_conditions(results)

    return results


def calculate_pre_trade_metrics(pre_candles, n_candles):
    """
    Calcule les m√©triques pour les bougies pr√©-trade
    """
    if len(pre_candles) != n_candles:
        return None

    # V√©rifier les colonnes n√©cessaires
    required_cols = ['sc_candleDuration', 'sc_volume_perTick']
    if not all(col in pre_candles.columns for col in required_cols):
        return None

    try:
        # M√©triques de base
        conditions = {
            # Dur√©e des bougies
            'duration_mean': pre_candles['sc_candleDuration'].mean(),
            'duration_median': pre_candles['sc_candleDuration'].median(),
            'duration_std': pre_candles['sc_candleDuration'].std(),
            'duration_min': pre_candles['sc_candleDuration'].min(),
            'duration_max': pre_candles['sc_candleDuration'].max(),

            # Volume par tick
            'volume_per_tick_mean': pre_candles['sc_volume_perTick'].mean(),
            'volume_per_tick_median': pre_candles['sc_volume_perTick'].median(),
            'volume_per_tick_std': pre_candles['sc_volume_perTick'].std(),
            'volume_per_tick_min': pre_candles['sc_volume_perTick'].min(),
            'volume_per_tick_max': pre_candles['sc_volume_perTick'].max(),

            # Vitesse des bougies
            'fast_candles_pct': (pre_candles['sc_candleDuration'] < 10).mean() * 100,
            'slow_candles_pct': (pre_candles['sc_candleDuration'] > 300).mean() * 100,

            # Volume des bougies (si disponible)
            'volume_candle_mean': None,
            'volume_candle_std': None,
        }

        # Volume des bougies si disponible
        if 'sc_volume' in pre_candles.columns:
            conditions['volume_candle_mean'] = pre_candles['sc_volume'].mean()
            conditions['volume_candle_std'] = pre_candles['sc_volume'].std()

        # ATR si disponible
        if 'sc_atr' in pre_candles.columns:
            conditions['atr_mean'] = pre_candles['sc_atr'].mean()
            conditions['atr_std'] = pre_candles['sc_atr'].std()
        else:
            conditions['atr_mean'] = None
            conditions['atr_std'] = None

        # Tendance des dur√©es (acc√©l√©ration/d√©c√©l√©ration)
        durations = pre_candles['sc_candleDuration'].values
        if len(durations) >= 3:
            # Calculer la tendance (r√©gression lin√©aire simple)
            x = np.arange(len(durations))
            trend_slope = np.polyfit(x, durations, 1)[0]
            conditions['duration_trend'] = trend_slope
        else:
            conditions['duration_trend'] = 0

        # Volatilit√© des volumes
        volumes = pre_candles['sc_volume_perTick'].values
        if len(volumes) >= 2:
            conditions['volume_volatility'] = np.std(volumes) / np.mean(volumes) if np.mean(volumes) > 0 else 0
        else:
            conditions['volume_volatility'] = 0

        return conditions

    except Exception as e:
        print(f"   ‚ö†Ô∏è Erreur calcul m√©triques: {e}")
        return None


def aggregate_pre_trade_conditions(conditions_list, dataset_name):
    """
    Agr√®ge les conditions pr√©-trade pour un dataset
    """
    if not conditions_list:
        return None

    # Convertir en DataFrame pour faciliter les calculs
    df_conditions = pd.DataFrame(conditions_list)

    # Calculer les statistiques agr√©g√©es
    aggregated = {}

    for col in df_conditions.columns:
        if df_conditions[col].dtype in ['int64', 'float64']:
            aggregated[f'{col}_mean'] = df_conditions[col].mean()
            aggregated[f'{col}_std'] = df_conditions[col].std()
            aggregated[f'{col}_median'] = df_conditions[col].median()
            aggregated[f'{col}_q25'] = df_conditions[col].quantile(0.25)
            aggregated[f'{col}_q75'] = df_conditions[col].quantile(0.75)

    aggregated['n_trades_analyzed'] = len(conditions_list)

    return aggregated


def display_pre_trade_analysis(aggregated, dataset_name):
    """
    Affiche l'analyse des conditions pr√©-trade
    """
    if not aggregated:
        print(f"‚ùå Pas de donn√©es pour {dataset_name}")
        return

    print(f"Trades analys√©s: {aggregated['n_trades_analyzed']}")

    print(f"\nüïê DUR√âE DES BOUGIES PR√â-TRADE:")
    print(f"   Moyenne: {aggregated['duration_mean_mean']:6.1f}s ¬± {aggregated['duration_mean_std']:5.1f}s")
    print(f"   M√©diane: {aggregated['duration_median_mean']:6.1f}s")
    print(f"   Min/Max: {aggregated['duration_min_mean']:6.1f}s / {aggregated['duration_max_mean']:6.1f}s")
    print(f"   Variabilit√©: {aggregated['duration_std_mean']:6.1f}s")
    print(f"   Tendance: {aggregated['duration_trend_mean']:+6.2f}s/bougie")

    print(f"\nüìä VOLUME PAR TICK PR√â-TRADE:")
    print(f"   Moyenne: {aggregated['volume_per_tick_mean_mean']:6.1f} ¬± {aggregated['volume_per_tick_mean_std']:5.1f}")
    print(f"   M√©diane: {aggregated['volume_per_tick_median_mean']:6.1f}")
    print(f"   Volatilit√©: {aggregated['volume_volatility_mean']:6.3f}")

    print(f"\n‚ö° VITESSE DES BOUGIES PR√â-TRADE:")
    print(f"   Rapides (<10s): {aggregated['fast_candles_pct_mean']:5.1f}%")
    print(f"   Lentes (>5min): {aggregated['slow_candles_pct_mean']:5.1f}%")

    if aggregated.get('atr_mean_mean'):
        print(f"\nüå™Ô∏è ATR PR√â-TRADE:")
        print(f"   Moyenne: {aggregated['atr_mean_mean']:6.3f} ¬± {aggregated['atr_mean_std']:6.3f}")


def compare_pre_trade_conditions(results):
    """
    Compare les conditions pr√©-trade entre datasets
    """
    print(f"\nüîç COMPARAISON DES CONDITIONS PR√â-TRADE ENTRE DATASETS")
    print("=" * 80)

    if 'TEST' not in results:
        print("‚ùå TEST non trouv√© dans les r√©sultats")
        return

    test_results = results['TEST']
    other_datasets = {k: v for k, v in results.items() if k != 'TEST'}

    print(f"\nüìä ANALYSE COMPARATIVE (TEST vs AUTRES)")
    print("-" * 60)

    # M√©triques cl√©s √† comparer
    key_metrics = [
        ('duration_mean_mean', 'Dur√©e moyenne', 's'),
        ('duration_std_mean', 'Variabilit√© dur√©e', 's'),
        ('volume_per_tick_mean_mean', 'Volume/tick moyen', ''),
        ('volume_volatility_mean', 'Volatilit√© volume', ''),
        ('fast_candles_pct_mean', 'Bougies rapides', '%'),
        ('duration_trend_mean', 'Tendance dur√©e', 's/bougie'),
    ]

    significant_differences = []

    for metric, description, unit in key_metrics:
        if metric not in test_results:
            continue

        test_value = test_results[metric]

        # Calculer moyenne des autres datasets
        other_values = []
        for name, data in other_datasets.items():
            if metric in data:
                other_values.append(data[metric])

        if not other_values:
            continue

        others_mean = np.mean(other_values)
        difference_pct = ((test_value - others_mean) / others_mean * 100) if others_mean != 0 else 0

        # Seuil de significativit√©
        is_significant = abs(difference_pct) > 10

        status = "üî¥" if abs(difference_pct) > 25 else "üü°" if abs(difference_pct) > 15 else "üü¢"
        direction = "plus √©lev√©" if difference_pct > 0 else "plus bas"

        print(
            f"{status} {description:20}: TEST {direction} de {abs(difference_pct):5.1f}% ({test_value:.2f}{unit} vs {others_mean:.2f}{unit})")

        if is_significant:
            significant_differences.append({
                'metric': description,
                'test_value': test_value,
                'others_mean': others_mean,
                'difference_pct': difference_pct,
                'unit': unit
            })

    # Analyse d√©taill√©e des diff√©rences significatives
    if significant_differences:
        print(f"\nüö® DIFF√âRENCES SIGNIFICATIVES D√âTECT√âES:")
        print("-" * 60)

        for diff in significant_differences:
            print(f"\nüìç {diff['metric']}:")
            print(f"   TEST: {diff['test_value']:.2f}{diff['unit']}")
            print(f"   AUTRES: {diff['others_mean']:.2f}{diff['unit']}")
            print(f"   √âCART: {diff['difference_pct']:+.1f}%")

            # Interpr√©tation
            if 'dur√©e' in diff['metric'].lower():
                if diff['difference_pct'] > 0:
                    print(f"   ‚ûú TEST op√®re sur des bougies plus lentes avant trade")
                else:
                    print(f"   ‚ûú TEST op√®re sur des bougies plus rapides avant trade")
            elif 'volume' in diff['metric'].lower():
                if diff['difference_pct'] > 0:
                    print(f"   ‚ûú TEST trade dans des conditions de volume plus √©lev√©")
                else:
                    print(f"   ‚ûú TEST trade dans des conditions de volume plus faible")
            elif 'tendance' in diff['metric'].lower():
                if diff['difference_pct'] > 0:
                    print(f"   ‚ûú TEST trade quand les bougies s'acc√©l√®rent")
                else:
                    print(f"   ‚ûú TEST trade quand les bougies ralentissent")

    # Diagnostic final
    print(f"\nüí° DIAGNOSTIC PR√â-TRADE:")
    print("-" * 40)

    if len(significant_differences) == 0:
        print("‚úÖ Les conditions pr√©-trade de TEST sont similaires aux autres")
        print("   ‚Üí Le probl√®me n'est probablement PAS dans les conditions d'entr√©e")
        print("   ‚Üí Chercher dans les signaux, le timing d'ex√©cution, ou la gestion")
    else:
        print(f"‚ö†Ô∏è {len(significant_differences)} diff√©rences majeures d√©tect√©es")
        print("   ‚Üí Les conditions pr√©-trade de TEST sont distinctes")
        print("   ‚Üí Ces diff√©rences peuvent expliquer la sous-performance")

        # Recommandations bas√©es sur les diff√©rences
        for diff in significant_differences:
            if 'rapides' in diff['metric'].lower() and diff['difference_pct'] > 0:
                print("   ‚Üí Recommandation: Filtrer les p√©riodes de bougies trop rapides")
            elif 'variabilit√©' in diff['metric'].lower() and diff['difference_pct'] > 0:
                print("   ‚Üí Recommandation: √âviter les p√©riodes trop volatiles")


def analyze_pre_trade_by_performance(datasets_dict, n_candles_before=10):
    """
    Analyse sp√©cifique: comparer les conditions pr√©-trade des wins vs losses
    """
    print(f"\nüéØ ANALYSE PR√â-TRADE: WINS vs LOSSES")
    print("=" * 60)

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_complete.empty or df_filtered.empty:
            continue

        print(f"\nüìä {name}:")
        print("-" * 30)

        # S√©parer wins et losses
        wins = df_filtered[df_filtered['class_binaire'] == 1]
        losses = df_filtered[df_filtered['class_binaire'] == 0]

        print(f"Wins: {len(wins)}, Losses: {len(losses)}")

        # Analyser les conditions pr√©-trade pour chaque groupe
        for group_name, group_data in [("WINS", wins), ("LOSSES", losses)]:
            if len(group_data) < 10:  # Minimum de trades pour l'analyse
                continue

            # √âchantillonner si trop de donn√©es
            sample_size = min(500, len(group_data))
            sample_data = group_data.sample(n=sample_size, random_state=42)

            pre_conditions = []

            for _, trade_row in sample_data.iterrows():
                # Trouver la position dans df_complete
                df_complete_sorted = df_complete.sort_values(['session_id', 'date'])

                matching_rows = df_complete_sorted[
                    (df_complete_sorted['date'] == trade_row['date']) &
                    (df_complete_sorted['session_id'] == trade_row['session_id'])
                    ]

                if len(matching_rows) > 0:
                    trade_idx = matching_rows.index[0]
                    complete_idx = df_complete_sorted.index.get_loc(trade_idx)

                    if complete_idx >= n_candles_before:
                        start_idx = complete_idx - n_candles_before
                        pre_candles = df_complete_sorted.iloc[start_idx:complete_idx]

                        conditions = calculate_pre_trade_metrics(pre_candles, n_candles_before)
                        if conditions:
                            pre_conditions.append(conditions)

            if pre_conditions:
                avg_conditions = aggregate_pre_trade_conditions(pre_conditions, f"{name}_{group_name}")

                if avg_conditions:
                    print(f"\n{group_name} ({len(pre_conditions)} √©chantillons):")
                    print(f"  Dur√©e moy: {avg_conditions['duration_mean_mean']:5.1f}s")
                    print(f"  Vol/tick:  {avg_conditions['volume_per_tick_mean_mean']:5.1f}")
                    print(f"  Rapides:   {avg_conditions['fast_candles_pct_mean']:4.1f}%")


# Fonction principale √† int√©grer dans le script principal
def add_pre_trade_analysis_to_main(datasets_dict, n_candles_before=10):
    """
    Fonction √† ajouter dans la fonction main() du script principal
    """

    # Analyse des conditions pr√©-trade
    print("\n" + "=" * 80)
    print("üîç ANALYSE DES CONDITIONS PR√â-TRADE")
    print("=" * 80)

    pre_trade_results = analyze_pre_trade_conditions(datasets_dict, n_candles_before)

    # Analyse wins vs losses
    analyze_pre_trade_by_performance(datasets_dict, n_candles_before)

    return pre_trade_results


import pandas as pd
import numpy as np
from pathlib import Path

# ===== √âTAPE 1: SUPPRIMER COMPL√àTEMENT calculate_slopes_pretrade_numba =====

# ‚ùå SUPPRIMEZ CETTE FONCTION ENTI√àREMENT DE VOTRE CODE :
"""
@njit
def calculate_slopes_pretrade_numba(values, window=10):
    # FONCTION √Ä SUPPRIMER - ELLE FAIT DU CLIPPING FORC√â
    pass
"""


# ===== √âTAPE 2: FONCTION DE REMPLACEMENT UNIQUE =====
def calculate_pre_trade_slopes_CLEAN(pre_candles, debug_mode=False):
    """
    Version PROPRE utilisant UNIQUEMENT calculate_slopes_and_r2_numba
    Plus de duplication, plus de clipping forc√©
    """
    try:
        durations = pre_candles['sc_candleDuration'].values
        volumes = pre_candles['sc_volume_perTick'].values

        if len(durations) < 3 or len(volumes) < 3:
            return {
                'duration_slope': 0, 'duration_r2': 0, 'duration_std': 0,
                'volume_slope': 0, 'volume_r2': 0, 'volume_std': 0
            }

        # Cr√©er session_starts (premi√®re bougie = d√©but de session)
        session_starts_dur = np.zeros(len(durations), dtype=bool)
        session_starts_vol = np.zeros(len(volumes), dtype=bool)
        session_starts_dur[0] = True
        session_starts_vol[0] = True

        if debug_mode:
            print(f"   üîç CLEAN: Calcul slopes avec calculate_slopes_and_r2_numba SANS clipping...")
            print(f"   üîç Dur√©es: {len(durations)} valeurs, range=[{np.min(durations):.1f}, {np.max(durations):.1f}]")
            print(f"   üîç Volumes: {len(volumes)} valeurs, range=[{np.min(volumes):.1f}, {np.max(volumes):.1f}]")

        # ‚úÖ UTILISATION UNIQUE DE VOTRE FONCTION OFFICIELLE
        duration_slopes, r2_dur, std_dur = calculate_slopes_and_r2_numba(
            durations, session_starts_dur, SLOPE_PERIODS,
            clip_slope=False,  # ‚Üê SANS CLIPPING
            include_close_bar=True
        )

        volume_slopes, r2_vol, std_vol = calculate_slopes_and_r2_numba(
            volumes, session_starts_vol, SLOPE_PERIODS,
            clip_slope=False,  # ‚Üê SANS CLIPPING
            include_close_bar=True
        )

        # Extraire les derni√®res valeurs valides
        dur_slope = 0
        vol_slope = 0
        dur_r2 = 0
        vol_r2 = 0
        dur_std = 0
        vol_std = 0

        if len(duration_slopes) > 0:
            valid_dur_slopes = duration_slopes[~np.isnan(duration_slopes)]
            if len(valid_dur_slopes) > 0:
                dur_slope = valid_dur_slopes[-1]
                dur_r2 = r2_dur[~np.isnan(duration_slopes)][-1] if len(r2_dur[~np.isnan(duration_slopes)]) > 0 else 0
                dur_std = std_dur[~np.isnan(duration_slopes)][-1] if len(std_dur[~np.isnan(duration_slopes)]) > 0 else 0

        if len(volume_slopes) > 0:
            valid_vol_slopes = volume_slopes[~np.isnan(volume_slopes)]
            if len(valid_vol_slopes) > 0:
                vol_slope = valid_vol_slopes[-1]
                vol_r2 = r2_vol[~np.isnan(volume_slopes)][-1] if len(r2_vol[~np.isnan(volume_slopes)]) > 0 else 0
                vol_std = std_vol[~np.isnan(volume_slopes)][-1] if len(std_vol[~np.isnan(volume_slopes)]) > 0 else 0

        if debug_mode:
            print(f"   ‚úÖ CLEAN: Slopes calcul√©es SANS clipping:")
            print(f"      Duration slope: {dur_slope:.6f} (R¬≤={dur_r2:.3f})")
            print(f"      Volume slope: {vol_slope:.6f} (R¬≤={vol_r2:.3f})")

        return {
            'duration_slope': dur_slope,
            'duration_r2': dur_r2,
            'duration_std': dur_std,
            'volume_slope': vol_slope,
            'volume_r2': vol_r2,
            'volume_std': vol_std
        }

    except Exception as e:
        if debug_mode:
            print(f"   ‚ùå ERREUR calculate_slopes_and_r2_numba: {e}")

        # Fallback numpy simple
        try:
            x = np.arange(len(durations))
            dur_slope = np.polyfit(x, durations, 1)[0] if len(durations) >= 3 else 0
            vol_slope = np.polyfit(x, volumes, 1)[0] if len(volumes) >= 3 else 0

            if debug_mode:
                print(f"   üîß FALLBACK numpy: dur={dur_slope:.6f}, vol={vol_slope:.6f}")

            return {
                'duration_slope': dur_slope, 'duration_r2': 0, 'duration_std': 0,
                'volume_slope': vol_slope, 'volume_r2': 0, 'volume_std': 0
            }
        except Exception as e2:
            if debug_mode:
                print(f"   ‚ùå ERREUR fallback: {e2}")
            return {
                'duration_slope': 0, 'duration_r2': 0, 'duration_std': 0,
                'volume_slope': 0, 'volume_r2': 0, 'volume_std': 0
            }



# ===== √âTAPE 3: FONCTION M√âTRIQUES NETTOY√âE =====

def calculate_pre_trade_metrics_CLEAN(pre_candles):
    """
    Version NETTOY√âE - utilise uniquement calculate_slopes_and_r2_numba
    """
    try:
        required_cols = ['sc_candleDuration', 'sc_volume_perTick']
        if not all(col in pre_candles.columns for col in required_cols):
            return None

        durations = pre_candles['sc_candleDuration'].values
        volumes = pre_candles['sc_volume_perTick'].values

        if len(durations) == 0 or len(volumes) == 0:
            return None
        if np.all(np.isnan(durations)) or np.all(np.isnan(volumes)):
            return None

        # Calculs de base
        stats = {
            'duration_mean': np.nanmean(durations),
            'duration_median': np.nanmedian(durations),
            'duration_std': np.nanstd(durations),
            'volume_mean': np.nanmean(volumes),
            'volume_median': np.nanmedian(volumes),
            'volume_std': np.nanstd(volumes),
            'fast_candles_pct': np.mean(durations < 10) * 100,
            'slow_candles_pct': np.mean(durations > 300) * 100,
        }

        # Coefficients de variation
        stats['duration_cv'] = stats['duration_std'] / stats['duration_mean'] if stats['duration_mean'] > 0 else 0
        stats['volume_cv'] = stats['volume_std'] / stats['volume_mean'] if stats['volume_mean'] > 0 else 0

        # ‚úÖ SLOPES AVEC LA FONCTION PROPRE
        if len(durations) >= 3:
            slope_results = calculate_pre_trade_slopes_CLEAN(pre_candles)

            stats['duration_trend'] = slope_results['duration_slope']
            stats['duration_slope_stdev'] = slope_results['duration_std']
            stats['duration_slope_r2'] = slope_results['duration_r2']
            stats['volume_trend'] = slope_results['volume_slope']
            stats['volume_slope_stdev'] = slope_results['volume_std']
            stats['volume_slope_r2'] = slope_results['volume_r2']
        else:
            return None

        # M√©triques suppl√©mentaires
        stats['duration_volatility'] = stats['duration_cv']
        stats['volume_volatility'] = stats['volume_cv']

        if len(durations) >= 5:
            mid = len(durations) // 2
            stats['duration_acceleration'] = np.mean(durations[mid:]) - np.mean(durations[:mid])
            stats['volume_acceleration'] = np.mean(volumes[mid:]) - np.mean(volumes[:mid])
        else:
            stats['duration_acceleration'] = 0
            stats['volume_acceleration'] = 0

        return stats

    except Exception as e:
        print(f"   ‚ùå Erreur calculate_pre_trade_metrics_CLEAN: {e}")
        return None


def analyze_pre_trade_conditions_complete(datasets_dict, n_candles_before=10):
    """
    Version COMPL√àTE OPTIMIS√âE - Analyse TOUS les trades sans √©chantillonnage
    """

    print(f"\nüîç ANALYSE COMPL√àTE DES {n_candles_before} BOUGIES PR√â-TRADE (TOUS LES TRADES)")
    print("=" * 80)

    results = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_complete.empty or df_filtered.empty:
            continue

        print(f"\nüìä DATASET: {name}")
        print("-" * 50)
        print(f"Analyse de {len(df_filtered):,} trades sur {len(df_complete):,} bougies...")

        # ===== OPTIMISATION MAJEURE 1: INDEX MULTI-NIVEAUX =====
        print("   Pr√©paration des index...")

        # Cr√©er un index optimis√© par session
        df_complete_reset = df_complete.reset_index(drop=True)
        df_complete_reset['row_number'] = df_complete_reset.index

        # Trier par session et date pour recherche s√©quentielle
        df_complete_sorted = df_complete_reset.sort_values(['session_id', 'date']).reset_index(drop=True)

        # ===== OPTIMISATION MAJEURE 2: GROUPBY PAR SESSION =====
        print("   Groupement par sessions...")

        # Grouper par session pour traitement parall√®le
        session_groups = df_complete_sorted.groupby('session_id')
        session_indices = {}

        for session_id, group in session_groups:
            session_indices[session_id] = {
                'data': group.reset_index(drop=True),
                'start_idx': group.index[0] if len(group) > 0 else 0
            }

        print(f"   {len(session_indices)} sessions index√©es")

        # ===== OPTIMISATION MAJEURE 3: TRAITEMENT VECTORIS√â =====
        print("   Traitement vectoris√© des trades...")

        pre_trade_stats = []
        total_trades = len(df_filtered)
        processed = 0
        skipped = 0

        # Grouper les trades par session aussi
        trades_by_session = df_filtered.groupby('session_id')

        for session_id, session_trades in trades_by_session:
            if session_id not in session_indices:
                skipped += len(session_trades)
                continue

            session_data = session_indices[session_id]['data']

            if len(session_data) < n_candles_before + 1:
                skipped += len(session_trades)
                continue

            # Traitement vectoris√© de tous les trades de cette session
            session_stats = process_session_trades_vectorized(
                session_trades, session_data, n_candles_before
            )

            pre_trade_stats.extend(session_stats)
            processed += len(session_trades)

            # Affichage progression
            if processed % 500 == 0 or processed == total_trades:
                pct = (processed / total_trades) * 100
                print(f"   Progression: {processed:,}/{total_trades:,} trades ({pct:.1f}%)")

        print(f"   ‚úÖ Analys√©s: {len(pre_trade_stats):,} | Ignor√©s: {skipped:,}")

        if pre_trade_stats:
            # Analyse des r√©sultats sans perte de donn√©es
            analysis = analyze_complete_stats(pre_trade_stats, name)
            results[name] = analysis
            display_complete_results(analysis, name)

    # Comparaison compl√®te
    if len(results) > 1:
        compare_datasets_complete(results)

    return results


def process_session_trades_vectorized(session_trades, session_data, n_candles_before):
    """
    Traitement vectoris√© optimis√© de tous les trades d'une session
    """
    session_stats = []

    # Convertir les dates en index pour recherche rapide
    session_dates = pd.to_datetime(session_data['date'])

    for _, trade_row in session_trades.iterrows():
        trade_date = pd.to_datetime(trade_row['date'])

        # Recherche vectoris√©e de la position
        # Trouver toutes les bougies avant ou √©gales √† la date du trade
        before_trade_mask = session_dates <= trade_date
        before_trade_indices = np.where(before_trade_mask)[0]

        if len(before_trade_indices) < n_candles_before + 1:
            continue

        # Position du trade (derni√®re bougie avant ou √©gale)
        trade_position = before_trade_indices[-1]

        # V√©rifier qu'on a assez de bougies avant
        if trade_position < n_candles_before:
            continue

        # Extraire les N bougies pr√©c√©dentes
        start_pos = trade_position - n_candles_before
        end_pos = trade_position

        pre_candles = session_data.iloc[start_pos:end_pos]

        if len(pre_candles) == n_candles_before:
            stats = calculate_pre_trade_metrics_CLEAN(pre_candles)
            if stats:
                stats['trade_result'] = trade_row['class_binaire']
                stats['session_id'] = trade_row['session_id']
                stats['trade_date'] = trade_row['date']
                session_stats.append(stats)

    return session_stats
def calculate_pre_trade_metrics_complete_debug_fixed(pre_candles):
    """
    Version corrig√©e utilisant DIRECTEMENT calculate_slopes_and_r2_numba
    """
    try:
        required_cols = ['sc_candleDuration', 'sc_volume_perTick']
        if not all(col in pre_candles.columns for col in required_cols):
            return None

        durations = pre_candles['sc_candleDuration'].values
        volumes = pre_candles['sc_volume_perTick'].values

        if len(durations) == 0 or len(volumes) == 0:
            return None
        if np.all(np.isnan(durations)) or np.all(np.isnan(volumes)):
            return None

        # Calculs de base
        stats = {
            'duration_mean': np.nanmean(durations),
            'duration_median': np.nanmedian(durations),
            'duration_std': np.nanstd(durations),
            'volume_mean': np.nanmean(volumes),
            'volume_median': np.nanmedian(volumes),
            'volume_std': np.nanstd(volumes),
            'fast_candles_pct': np.mean(durations < 10) * 100,
            'slow_candles_pct': np.mean(durations > 300) * 100,
        }

        # Coefficients de variation
        if stats['duration_mean'] > 0:
            stats['duration_cv'] = stats['duration_std'] / stats['duration_mean']
        else:
            stats['duration_cv'] = 0

        if stats['volume_mean'] > 0:
            stats['volume_cv'] = stats['volume_std'] / stats['volume_mean']
        else:
            stats['volume_cv'] = 0

        # ‚úÖ SLOPES AVEC VOTRE FONCTION OFFICIELLE calculate_slopes_and_r2_numba
        if len(durations) >= 3:
            # Cr√©er session_starts (premi√®re bougie = d√©but de session)
            session_starts_dur = np.zeros(len(durations), dtype=bool)
            session_starts_vol = np.zeros(len(volumes), dtype=bool)
            session_starts_dur[0] = True
            session_starts_vol[0] = True

            # Appel direct √† votre fonction
            duration_slopes, r2_dur, std_dur = calculate_slopes_and_r2_numba(
                durations, session_starts_dur, SLOPE_PERIODS,
                clip_slope=False,  # ‚Üê SANS CLIPPING
                include_close_bar=True
            )

            volume_slopes, r2_vol, std_vol = calculate_slopes_and_r2_numba(
                volumes, session_starts_vol, SLOPE_PERIODS,
                clip_slope=False,  # ‚Üê SANS CLIPPING
                include_close_bar=True
            )

            # Extraire les derni√®res valeurs valides
            dur_slope = 0
            vol_slope = 0
            dur_r2 = 0
            vol_r2 = 0
            dur_std = 0
            vol_std = 0

            if len(duration_slopes) > 0:
                valid_indices_dur = ~np.isnan(duration_slopes)
                if np.any(valid_indices_dur):
                    dur_slope = duration_slopes[valid_indices_dur][-1]
                    dur_r2 = r2_dur[valid_indices_dur][-1]
                    dur_std = std_dur[valid_indices_dur][-1]

            if len(volume_slopes) > 0:
                valid_indices_vol = ~np.isnan(volume_slopes)
                if np.any(valid_indices_vol):
                    vol_slope = volume_slopes[valid_indices_vol][-1]
                    vol_r2 = r2_vol[valid_indices_vol][-1]
                    vol_std = std_vol[valid_indices_vol][-1]

            stats['duration_trend'] = dur_slope
            stats['duration_slope_stdev'] = dur_std
            stats['duration_slope_r2'] = dur_r2
            stats['volume_trend'] = vol_slope
            stats['volume_slope_stdev'] = vol_std
            stats['volume_slope_r2'] = vol_r2

        else:
            return None

        # Autres m√©triques
        stats['duration_volatility'] = stats['duration_cv']
        stats['volume_volatility'] = stats['volume_cv']

        if len(durations) >= 5:
            mid = len(durations) // 2
            first_half_dur = np.mean(durations[:mid])
            second_half_dur = np.mean(durations[mid:])
            stats['duration_acceleration'] = second_half_dur - first_half_dur

            first_half_vol = np.mean(volumes[:mid])
            second_half_vol = np.mean(volumes[mid:])
            stats['volume_acceleration'] = second_half_vol - first_half_vol
        else:
            stats['duration_acceleration'] = 0
            stats['volume_acceleration'] = 0

        return stats

    except Exception as e:
        print(f"   ‚ùå Erreur calculate_pre_trade_metrics_complete_debug_fixed: {e}")
        return None

def process_session_trades_CLEAN(session_trades, session_data, n_candles_before):
    """
    Version nettoy√©e du traitement de session
    """
    session_stats = []

    session_dates_np = session_data['date'].values.astype('datetime64[ns]')

    for _, trade_row in session_trades.iterrows():
        trade_date_np = np.datetime64(trade_row['date'])

        mask = session_dates_np <= trade_date_np
        indices = np.where(mask)[0]

        if len(indices) < n_candles_before + 1:
            continue

        trade_position = indices[-1]

        if trade_position < n_candles_before:
            continue

        start_pos = trade_position - n_candles_before
        end_pos = trade_position

        pre_candles = session_data.iloc[start_pos:end_pos]

        if len(pre_candles) == n_candles_before:
            # ‚úÖ UTILISE LA FONCTION NETTOY√âE
            stats = calculate_pre_trade_metrics_CLEAN(pre_candles)
            if stats:
                stats.update({
                    'trade_result': trade_row['class_binaire'],
                    'session_id': trade_row['session_id'],
                    'trade_date': trade_row['date']
                })
                session_stats.append(stats)

    return session_stats


# ===== √âTAPE 6: FONCTION PRINCIPALE NETTOY√âE =====

def analyze_pre_trade_conditions_CLEAN(datasets_dict, n_candles_before=10):
    """
    Version NETTOY√âE de l'analyse pr√©-trade
    Utilise uniquement calculate_slopes_and_r2_numba
    """
    print(f"\nüßπ ANALYSE PR√â-TRADE NETTOY√âE (SANS DUPLICATION)")
    print("=" * 80)

    results = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_complete.empty or df_filtered.empty:
            continue

        print(f"\nüßπ DATASET CLEAN: {name}")
        print("-" * 50)

        # Pr√©paration optimis√©e
        df_complete_sorted = df_complete.sort_values(['session_id', 'date']).reset_index(drop=True)
        session_groups = df_complete_sorted.groupby('session_id', sort=False)
        session_indices = {session_id: group.reset_index(drop=True)
                          for session_id, group in session_groups}

        pre_trade_stats = []
        total_trades = len(df_filtered)
        processed = 0

        trades_by_session = df_filtered.groupby('session_id', sort=False)

        for session_id, session_trades in trades_by_session:
            if session_id not in session_indices:
                continue

            session_data = session_indices[session_id]
            if len(session_data) < n_candles_before + 1:
                continue

            # ‚úÖ TRAITEMENT NETTOY√â
            session_stats = process_session_trades_CLEAN(
                session_trades, session_data, n_candles_before
            )

            pre_trade_stats.extend(session_stats)
            processed += len(session_trades)

            if processed % 1000 == 0:
                pct = (processed / total_trades) * 100
                print(f"   üßπ Clean progression: {processed:,}/{total_trades:,} ({pct:.1f}%)")

        print(f"   ‚úÖ CLEAN: {len(pre_trade_stats):,} trades analys√©s")

        if pre_trade_stats:
            analysis = analyze_complete_stats_fast(pre_trade_stats, name)
            results[name] = analysis
            display_complete_results_fast(analysis, name)

    return results
def analyze_complete_stats(pre_trade_stats, dataset_name):
    """
    Analyse compl√®te sans perte d'information
    """
    df_stats = pd.DataFrame(pre_trade_stats)

    # S√©parer wins et losses
    wins = df_stats[df_stats['trade_result'] == 1]
    losses = df_stats[df_stats['trade_result'] == 0]

    analysis = {
        'dataset': dataset_name,
        'total_trades': len(df_stats),
        'wins_count': len(wins),
        'losses_count': len(losses),
        'winrate': len(wins) / len(df_stats) * 100 if len(df_stats) > 0 else 0,
        'overall': {},
        'wins': {},
        'losses': {},
        'differences': {},
        'raw_data': df_stats  # Garder les donn√©es brutes
    }

    # Toutes les m√©triques num√©riques
    numeric_cols = [col for col in df_stats.columns if df_stats[col].dtype in ['int64', 'float64']]
    numeric_cols = [col for col in numeric_cols if col != 'trade_result']  # Exclure le r√©sultat

    for col in numeric_cols:
        # Statistiques globales
        analysis['overall'][col] = {
            'mean': df_stats[col].mean(),
            'median': df_stats[col].median(),
            'std': df_stats[col].std(),
            'min': df_stats[col].min(),
            'max': df_stats[col].max(),
            'q25': df_stats[col].quantile(0.25),
            'q75': df_stats[col].quantile(0.75)
        }

        # Statistiques wins
        if len(wins) > 0:
            analysis['wins'][col] = {
                'mean': wins[col].mean(),
                'median': wins[col].median(),
                'std': wins[col].std()
            }

        # Statistiques losses
        if len(losses) > 0:
            analysis['losses'][col] = {
                'mean': losses[col].mean(),
                'median': losses[col].median(),
                'std': losses[col].std()
            }

        # Diff√©rences wins vs losses
        if len(wins) > 0 and len(losses) > 0:
            wins_mean = analysis['wins'][col]['mean']
            losses_mean = analysis['losses'][col]['mean']
            overall_mean = analysis['overall'][col]['mean']

            diff_abs = wins_mean - losses_mean
            diff_pct = (diff_abs / overall_mean * 100) if overall_mean != 0 else 0

            analysis['differences'][col] = {
                'abs': diff_abs,
                'pct': diff_pct,
                'wins_mean': wins_mean,
                'losses_mean': losses_mean
            }

    return analysis


def display_complete_results(analysis, dataset_name):
    """
    Affichage complet des r√©sultats - MODIFI√â pour inclure les infos slopes officielles
    """
    print(f"\nüìà R√âSULTATS COMPLETS {dataset_name}:")
    print(f"   Total trades: {analysis['total_trades']:,}")
    print(f"   Wins: {analysis['wins_count']:,} | Losses: {analysis['losses_count']:,}")
    print(f"   Winrate √©chantillon: {analysis['winrate']:.2f}%")

    # Conditions pr√©-trade moyennes
    if 'duration_mean' in analysis['overall']:
        dur_data = analysis['overall']['duration_mean']
        vol_data = analysis['overall']['volume_mean']

        print(f"\n   üïê CONDITIONS PR√â-TRADE MOYENNES:")
        print(f"   ‚Ä¢ Dur√©e: {dur_data['mean']:5.1f}s (m√©diane: {dur_data['median']:5.1f}s)")
        print(f"   ‚Ä¢ Volume/tick: {vol_data['mean']:5.1f} (m√©diane: {vol_data['median']:5.1f})")

        if 'fast_candles_pct' in analysis['overall']:
            fast_data = analysis['overall']['fast_candles_pct']
            slow_data = analysis['overall']['slow_candles_pct']
            print(f"   ‚Ä¢ Rapides (<10s): {fast_data['mean']:4.1f}%")
            print(f"   ‚Ä¢ Lentes (>5min): {slow_data['mean']:4.1f}%")

    # Top diff√©rences wins vs losses avec info slopes
    if analysis['differences']:
        print(f"\n   üéØ TOP DIFF√âRENCES WINS vs LOSSES:")

        # Trier par importance de la diff√©rence
        sorted_diffs = sorted(analysis['differences'].items(),
                              key=lambda x: abs(x[1]['pct']), reverse=True)

        significant_count = 0
        for metric, diff_data in sorted_diffs:
            if abs(diff_data['pct']) > 3:  # Seuil 3% pour √™tre inclusif
                significant_count += 1
                if significant_count <= 5:  # Top 5
                    direction = "plus √©lev√©" if diff_data['pct'] > 0 else "plus bas"

                    # Ajouter info sur la m√©thode de calcul pour les trends
                    method_info = ""
                    if 'trend' in metric:
                        method_info = f" (calculate_slopes_and_r2_numba, {SLOPE_PERIODS} p√©riodes)"

                    print(f"   ‚Ä¢ {metric}: WINS {direction} de {abs(diff_data['pct']):.1f}%{method_info}")

        if significant_count == 0:
            print(f"   ‚Ä¢ Aucune diff√©rence significative (>3%) d√©tect√©e")


# FONCTIONS MANQUANTES √Ä AJOUTER √Ä VOTRE SCRIPT

def analyze_wins_vs_losses_slopes(analysis_results):
    """
    Analyse d√©taill√©e WINS vs LOSSES sur les slopes
    FONCTION MANQUANTE CRITIQUE pour valider nos hypoth√®ses
    """
    print(f"\nüéØ ANALYSE SLOPES: WINS vs LOSSES (calculate_slopes_and_r2_numba)")
    print("=" * 70)

    slope_comparisons = {}

    for name, data in analysis_results.items():
        if 'raw_data' not in data or data['raw_data'].empty:
            continue

        df = data['raw_data']
        wins = df[df['trade_result'] == 1]
        losses = df[df['trade_result'] == 0]

        if len(wins) == 0 or len(losses) == 0:
            continue

        # Calculs des moyennes slopes
        wins_vol_trend = wins['volume_trend'].mean()
        losses_vol_trend = losses['volume_trend'].mean()
        wins_dur_trend = wins['duration_trend'].mean()
        losses_dur_trend = losses['duration_trend'].mean()

        # Calcul des diff√©rences en pourcentage
        vol_diff_abs = wins_vol_trend - losses_vol_trend
        dur_diff_abs = wins_dur_trend - losses_dur_trend

        # Pourcentages (par rapport √† la moyenne g√©n√©rale)
        overall_vol = df['volume_trend'].mean()
        overall_dur = df['duration_trend'].mean()

        vol_diff_pct = (vol_diff_abs / abs(overall_vol) * 100) if abs(overall_vol) > 0.001 else 0
        dur_diff_pct = (dur_diff_abs / abs(overall_dur) * 100) if abs(overall_dur) > 0.001 else 0

        slope_comparisons[name] = {
            'wins_vol_trend': wins_vol_trend,
            'losses_vol_trend': losses_vol_trend,
            'wins_dur_trend': wins_dur_trend,
            'losses_dur_trend': losses_dur_trend,
            'vol_diff_abs': vol_diff_abs,
            'dur_diff_abs': dur_diff_abs,
            'vol_diff_pct': vol_diff_pct,
            'dur_diff_pct': dur_diff_pct
        }

        print(f"\nüìä {name}:")
        print(f"   Volume trend - WINS: {wins_vol_trend:+.4f} | LOSSES: {losses_vol_trend:+.4f}")
        print(f"   ‚Üí Diff√©rence: WINS plus {'√©lev√©' if vol_diff_abs > 0 else 'bas'} de {abs(vol_diff_pct):.1f}%")

        print(f"   Duration trend - WINS: {wins_dur_trend:+.4f} | LOSSES: {losses_dur_trend:+.4f}")
        print(f"   ‚Üí Diff√©rence: WINS plus {'√©lev√©' if dur_diff_abs > 0 else 'bas'} de {abs(dur_diff_pct):.1f}%")

    return slope_comparisons


def validate_test_timing_hypothesis(slope_comparisons):
    """
    Valide sp√©cifiquement l'hypoth√®se du timing tardif de TEST
    """
    print(f"\nüö® VALIDATION HYPOTH√àSE: TEST trade en retard (√©puisement)")
    print("=" * 60)

    if 'TEST' not in slope_comparisons:
        print("‚ùå TEST non trouv√© dans les comparaisons")
        return

    test_data = slope_comparisons['TEST']
    other_datasets = {k: v for k, v in slope_comparisons.items() if k != 'TEST'}

    print(f"üìä PATTERN TEST:")
    print(f"   Volume trend WINS vs LOSSES: {test_data['vol_diff_pct']:+.1f}%")
    print(f"   Duration trend WINS vs LOSSES: {test_data['dur_diff_pct']:+.1f}%")

    # Calculer moyenne des autres datasets
    if other_datasets:
        other_vol_diffs = [data['vol_diff_pct'] for data in other_datasets.values()]
        other_dur_diffs = [data['dur_diff_pct'] for data in other_datasets.values()]

        avg_vol_diff_others = sum(other_vol_diffs) / len(other_vol_diffs)
        avg_dur_diff_others = sum(other_dur_diffs) / len(other_dur_diffs)

        print(f"\nüìä PATTERN AUTRES DATASETS (moyenne):")
        print(f"   Volume trend WINS vs LOSSES: {avg_vol_diff_others:+.1f}%")
        print(f"   Duration trend WINS vs LOSSES: {avg_dur_diff_others:+.1f}%")

        print(f"\nüéØ COMPARAISON TEST vs AUTRES:")

        # Test de l'hypoth√®se volume
        vol_divergence = test_data['vol_diff_pct'] - avg_vol_diff_others
        print(f"   Volume trend divergence: {vol_divergence:+.1f}%")

        if test_data['vol_diff_pct'] > 0 and avg_vol_diff_others < 0:
            print("   ‚úÖ HYPOTH√àSE CONFIRM√âE: TEST trade sur volume croissant vs autres sur volume d√©croissant")
            print("   ‚Üí TEST entre tard (√©puisement) vs autres entrent t√¥t (accumulation)")
        elif abs(vol_divergence) > 50:
            print(f"   ‚ö†Ô∏è  FORTE DIVERGENCE: TEST a un pattern volume tr√®s diff√©rent")
        else:
            print("   üî∂ Pas de divergence majeure sur le volume trend")

        # Test de l'hypoth√®se dur√©e
        dur_divergence = test_data['dur_diff_pct'] - avg_dur_diff_others
        print(f"   Duration trend divergence: {dur_divergence:+.1f}%")

        if test_data['dur_diff_pct'] > 0 and avg_dur_diff_others < 0:
            print("   ‚úÖ HYPOTH√àSE CONFIRM√âE: TEST trade sur bougies qui ralentissent vs autres sur acc√©l√©ration")
        elif abs(dur_divergence) > 50:
            print(f"   ‚ö†Ô∏è  FORTE DIVERGENCE: TEST a un pattern dur√©e tr√®s diff√©rent")
        else:
            print("   üî∂ Pas de divergence majeure sur le duration trend")

        # Diagnostic final
        print(f"\nüí° DIAGNOSTIC FINAL:")

        vol_confirmed = test_data['vol_diff_pct'] > 0 and avg_vol_diff_others < 0
        dur_confirmed = test_data['dur_diff_pct'] > 0 and avg_dur_diff_others < 0

        if vol_confirmed and dur_confirmed:
            print("   üî¥ TIMING TARDIF CONFIRM√â: TEST entre en phase d'√©puisement")
            print("   ‚Üí Volume qui monte (participation tardive) + bougies qui ralentissent")
            print("   ‚Üí Recommandation: Filtrer ces conditions ou inverser la logique")
        elif vol_confirmed or dur_confirmed:
            print("   üü° TIMING TARDIF PARTIELLEMENT CONFIRM√â")
            print("   ‚Üí Une des deux conditions d'√©puisement d√©tect√©e")
        else:
            print("   üü¢ TIMING TARDIF NON CONFIRM√â")
            print("   ‚Üí Les patterns slopes ne confirment pas l'hypoth√®se d'√©puisement")


def compare_slope_patterns_detailed(slope_comparisons):
    """
    Comparaison d√©taill√©e des patterns slopes entre tous les datasets
    """
    print(f"\nüìà COMPARAISON D√âTAILL√âE DES PATTERNS SLOPES")
    print("=" * 70)

    # Tableau de comparaison
    print(f"{'Dataset':<15} {'Vol.Trend':<10} {'Dur.Trend':<10} {'Interpr√©tation':<30}")
    print("-" * 70)

    for name, data in slope_comparisons.items():
        vol_direction = "+" if data['vol_diff_pct'] > 0 else "-"
        dur_direction = "+" if data['dur_diff_pct'] > 0 else "-"

        # Classification du pattern
        if data['vol_diff_pct'] > 0 and data['dur_diff_pct'] < 0:
            pattern = "Volume‚Üë Dur√©e‚Üì (Momentum)"
        elif data['vol_diff_pct'] < 0 and data['dur_diff_pct'] < 0:
            pattern = "Volume‚Üì Dur√©e‚Üì (Spring)"
        elif data['vol_diff_pct'] > 0 and data['dur_diff_pct'] > 0:
            pattern = "Volume‚Üë Dur√©e‚Üë (√âpuisement)"
        elif data['vol_diff_pct'] < 0 and data['dur_diff_pct'] > 0:
            pattern = "Volume‚Üì Dur√©e‚Üë (Consolidation)"
        else:
            pattern = "Pattern neutre"

        print(
            f"{name:<15} {vol_direction}{abs(data['vol_diff_pct']):>4.1f}%    {dur_direction}{abs(data['dur_diff_pct']):>4.1f}%    {pattern:<30}")

    # Recommandations par pattern
    print(f"\nüí° RECOMMANDATIONS PAR PATTERN:")
    print("-" * 40)

    for name, data in slope_comparisons.items():
        if data['vol_diff_pct'] > 0 and data['dur_diff_pct'] > 0:
            print(f"üî¥ {name}: √âVITER - Pattern d'√©puisement d√©tect√©")
        elif data['vol_diff_pct'] < 0 and data['dur_diff_pct'] < 0:
            print(f"üü¢ {name}: OPTIMAL - Pattern spring/compression")
        elif data['vol_diff_pct'] > 0 and data['dur_diff_pct'] < 0:
            print(f"üü° {name}: BON - Pattern momentum classique")
        else:
            print(f"üî∂ {name}: MOYEN - Pattern consolidation")


# FONCTION PRINCIPALE √Ä AJOUTER DANS main()
def add_missing_slopes_analysis(results):
    """
    Ajoute l'analyse manquante des slopes
    √Ä appeler apr√®s add_ultra_fast_pre_trade_analysis()
    """
    print(f"\n" + "=" * 80)
    print("üîç ANALYSE SLOPES D√âTAILL√âE - VALIDATION HYPOTH√àSES")
    print("=" * 80)

    # Analyse wins vs losses slopes
    slope_comparisons = analyze_wins_vs_losses_slopes(results)

    # Validation hypoth√®se TEST
    validate_test_timing_hypothesis(slope_comparisons)

    # Comparaison d√©taill√©e
    compare_slope_patterns_detailed(slope_comparisons)

    return slope_comparisons


def compare_datasets_complete(results):
    """
    Comparaison compl√®te entre tous les datasets
    """
    print(f"\nüîç COMPARAISON COMPL√àTE DES CONDITIONS PR√â-TRADE")
    print("=" * 70)

    if 'TEST' not in results:
        print("‚ùå TEST introuvable dans les r√©sultats")
        return

    test_data = results['TEST']['overall']
    other_datasets = {k: v for k, v in results.items() if k != 'TEST'}

    print(f"\nüìä TEST vs AUTRES DATASETS:")
    print("-" * 50)

    # M√©triques principales pour comparaison
    key_metrics = [
        ('duration_mean', 'Dur√©e moyenne pr√©-trade', 's'),
        ('duration_std', 'Variabilit√© dur√©e pr√©-trade', 's'),
        ('volume_mean', 'Volume/tick moyen pr√©-trade', ''),
        ('volume_volatility', 'Volatilit√© volume pr√©-trade', ''),
        ('fast_candles_pct', 'Bougies rapides pr√©-trade', '%'),
        ('slow_candles_pct', 'Bougies lentes pr√©-trade', '%'),
        ('duration_trend', 'Tendance dur√©e pr√©-trade', 's/bougie'),
        ('duration_acceleration', 'Acc√©l√©ration dur√©e pr√©-trade', 's')
    ]

    significant_differences = []

    for metric, description, unit in key_metrics:
        if metric not in test_data or 'mean' not in test_data[metric]:
            continue

        test_value = test_data[metric]['mean']

        # Calculer moyenne des autres datasets
        other_values = []
        for name, data in other_datasets.items():
            if metric in data['overall'] and 'mean' in data['overall'][metric]:
                other_values.append(data['overall'][metric]['mean'])

        if not other_values:
            continue

        others_mean = np.mean(other_values)
        others_std = np.std(other_values)

        # Calculer l'√©cart
        if others_mean != 0:
            difference_pct = (test_value - others_mean) / others_mean * 100
        else:
            difference_pct = 0

        # D√©terminer la significativit√©
        is_very_significant = abs(difference_pct) > 25
        is_significant = abs(difference_pct) > 15
        is_notable = abs(difference_pct) > 10

        if is_very_significant:
            status = "üî¥"
        elif is_significant:
            status = "üü°"
        elif is_notable:
            status = "üü†"
        else:
            status = "üü¢"

        direction = "plus √©lev√©" if difference_pct > 0 else "plus bas"

        print(f"{status} {description:30}: TEST {direction} de {abs(difference_pct):5.1f}%")
        print(f"    TEST: {test_value:.2f}{unit} | AUTRES: {others_mean:.2f}¬±{others_std:.2f}{unit}")

        if is_significant:
            significant_differences.append({
                'metric': description,
                'test_value': test_value,
                'others_mean': others_mean,
                'difference_pct': difference_pct,
                'unit': unit
            })

    # Synth√®se des diff√©rences majeures
    if significant_differences:
        print(f"\nüö® SYNTH√àSE DES DIFF√âRENCES MAJEURES:")
        print("-" * 50)

        for diff in significant_differences:
            print(f"\nüìç {diff['metric']}:")
            print(f"   √âcart: {diff['difference_pct']:+.1f}%")

            # Interpr√©tation business
            if 'dur√©e' in diff['metric'].lower() and 'moyenne' in diff['metric'].lower():
                if diff['difference_pct'] > 0:
                    print(f"   ‚Üí TEST trade apr√®s des s√©quences de bougies plus lentes")
                    print(f"   ‚Üí Possible attente trop longue ou h√©sitation avant signal")
                else:
                    print(f"   ‚Üí TEST trade apr√®s des s√©quences de bougies plus rapides")
                    print(f"   ‚Üí Possible r√©action impulsive ou trading dans la nervosit√©")

            elif 'rapides' in diff['metric'].lower():
                if diff['difference_pct'] > 0:
                    print(f"   ‚Üí TEST trade plus souvent apr√®s des p√©riodes agit√©es")
                    print(f"   ‚Üí Risque de signaux dans le bruit de march√©")
                else:
                    print(f"   ‚Üí TEST trade plus souvent apr√®s des p√©riodes calmes")
                    print(f"   ‚Üí Meilleur timing potentiel")

            elif 'tendance' in diff['metric'].lower():
                if diff['difference_pct'] > 0:
                    print(f"   ‚Üí TEST trade quand le march√© s'acc√©l√®re")
                    print(f"   ‚Üí Possible entr√©e tardive dans le mouvement")
                else:
                    print(f"   ‚Üí TEST trade quand le march√© ralentit")
                    print(f"   ‚Üí Possible anticipation ou retournement")

    else:
        print(f"\n‚úÖ AUCUNE DIFF√âRENCE MAJEURE PR√â-TRADE D√âTECT√âE")
        print("   ‚Üí Les conditions d'entr√©e de TEST sont similaires aux autres")
        print("   ‚Üí Le probl√®me est probablement ailleurs:")
        print("     ‚Ä¢ Signaux ou algorithme de s√©lection")
        print("     ‚Ä¢ Timing d'ex√©cution pr√©cis")
        print("     ‚Ä¢ Gestion des trades (stops/TP)")
        print("     ‚Ä¢ Facteurs post-entr√©e")


def analyze_complete_stats_fast(pre_trade_stats, dataset_name):
    """Version rapide de l'analyse des stats"""
    # Conversion ultra-rapide en array numpy
    data_array = []
    for stat in pre_trade_stats:
        row = [
            stat.get('duration_mean', 0),
            stat.get('volume_mean', 0),
            stat.get('duration_trend', 0),
            stat.get('volume_trend', 0),
            stat.get('trade_result', 0)
        ]
        data_array.append(row)

    data_np = np.array(data_array)

    # S√©paration vectoris√©e wins/losses
    wins_mask = data_np[:, 4] == 1
    losses_mask = data_np[:, 4] == 0

    wins_data = data_np[wins_mask]
    losses_data = data_np[losses_mask]

    analysis = {
        'dataset': dataset_name,
        'total_trades': len(data_np),
        'wins_count': len(wins_data),
        'losses_count': len(losses_data),
        'winrate': len(wins_data) / len(data_np) * 100 if len(data_np) > 0 else 0,
        'raw_data': pd.DataFrame(pre_trade_stats)  # Conversion finale seulement
    }

    return analysis


def display_complete_results_fast(analysis, dataset_name):
    """Affichage rapide des r√©sultats"""
    print(f"\n‚ö° R√âSULTATS ULTRA-RAPIDES {dataset_name}:")
    print(f"   Trades: {analysis['total_trades']:,} | Winrate: {analysis['winrate']:.2f}%")


def calculate_pre_trade_slopes_debug(pre_candles):
    """
    Version DEBUG pour identifier pourquoi les slopes sont NaN
    """
    print(f"   üîç DEBUG: Analyse des donn√©es pr√©-trade...")

    durations = pre_candles['sc_candleDuration'].values
    volumes = pre_candles['sc_volume_perTick'].values

    print(f"   üîç Dur√©es: {len(durations)} valeurs, min={np.nanmin(durations):.2f}, max={np.nanmax(durations):.2f}")
    print(f"   üîç Volumes: {len(volumes)} valeurs, min={np.nanmin(volumes):.2f}, max={np.nanmax(volumes):.2f}")

    # V√©rifier session_starts
    if 'sc_sessionStartEnd' in pre_candles.columns:
        session_starts = (pre_candles['sc_sessionStartEnd'] == 10).values
        print(f"   üîç Session starts: {session_starts.sum()} sur {len(session_starts)} (valeurs True)")
    else:
        print(f"   ‚ùå Colonne 'sc_sessionStartEnd' manquante !")
        session_starts = np.zeros(len(durations), dtype=bool)
        session_starts[0] = True  # Forcer au moins un d√©but de session
        print(f"   üîß Session starts forc√©e: 1 d√©but de session cr√©√©")

    try:
        # Tenter avec votre fonction
        print(f"   üîç Appel calculate_slopes_and_r2_numba...")
        duration_slopes, r2_dur, std_dur = calculate_slopes_and_r2_numba(
            durations, session_starts, SLOPE_PERIODS,  clip_slope=False,
                                  include_close_bar=True)

        print(f"   üîç R√©sultats dur√©e: slopes={len(duration_slopes)}, r2={len(r2_dur)}, std={len(std_dur)}")
        if len(duration_slopes) > 0:
            print(f"   üîç Premi√®re slope dur√©e: {duration_slopes[0]}")
            print(f"   üîç Derni√®re slope dur√©e: {duration_slopes[-1]}")

        volume_slopes, r2_vol, std_vol = calculate_slopes_and_r2_numba(
            volumes, session_starts, SLOPE_PERIODS,  clip_slope=False,
                                  include_close_bar=True)


        print(f"   üîç R√©sultats volume: slopes={len(volume_slopes)}, r2={len(r2_vol)}, std={len(std_vol)}")
        if len(volume_slopes) > 0:
            print(f"   üîç Premi√®re slope volume: {volume_slopes[0]}")
            print(f"   üîç Derni√®re slope volume: {volume_slopes[-1]}")

        # Retourner la derni√®re slope valide
        duration_slope = duration_slopes[-1] if len(duration_slopes) > 0 and not np.isnan(duration_slopes[-1]) else 0
        volume_slope = volume_slopes[-1] if len(volume_slopes) > 0 and not np.isnan(volume_slopes[-1]) else 0

        return {
            'duration_slope': duration_slope,
            'duration_r2': r2_dur[-1] if len(r2_dur) > 0 else 0,
            'duration_std': std_dur[-1] if len(std_dur) > 0 else 0,
            'volume_slope': volume_slope,
            'volume_r2': r2_vol[-1] if len(r2_vol) > 0 else 0,
            'volume_std': std_vol[-1] if len(std_vol) > 0 else 0
        }

    except Exception as e:
        print(f"   ‚ùå ERREUR calculate_slopes_and_r2_numba: {e}")

        # FALLBACK: Utiliser numpy pour debug
        print(f"   üîß FALLBACK: Utilisation numpy.polyfit...")
        try:
            x = np.arange(len(durations))
            duration_slope = np.polyfit(x, durations, 1)[0] if len(durations) >= 3 else 0
            volume_slope = np.polyfit(x, volumes, 1)[0] if len(volumes) >= 3 else 0

            print(f"   üîß Slope dur√©e (numpy): {duration_slope}")
            print(f"   üîß Slope volume (numpy): {volume_slope}")

            return {
                'duration_slope': duration_slope,
                'duration_r2': 0,
                'duration_std': 0,
                'volume_slope': volume_slope,
                'volume_r2': 0,
                'volume_std': 0
            }
        except Exception as e2:
            print(f"   ‚ùå ERREUR numpy aussi: {e2}")
            return {
                'duration_slope': 0, 'duration_r2': 0, 'duration_std': 0,
                'volume_slope': 0, 'volume_r2': 0, 'volume_std': 0
            }


@njit
def fast_stats_calculation(values):
    """Calcul ultra-rapide des statistiques de base avec numba"""
    if len(values) == 0:
        return np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])

    mean_val = np.mean(values)
    median_val = np.median(values)
    std_val = np.std(values)
    min_val = np.min(values)
    max_val = np.max(values)
    q25_val = fast_percentile(values, 25)
    q75_val = fast_percentile(values, 75)

    return np.array([mean_val, median_val, std_val, min_val, max_val, q25_val, q75_val])
@njit
def fast_candle_classification(durations):
    """Classification rapide des types de bougies"""
    if len(durations) == 0:
        return np.array([0.0, 0.0, 0.0])

    fast_count = np.sum(durations < 10)
    slow_count = np.sum(durations > 300)
    medium_count = len(durations) - fast_count - slow_count

    total = len(durations)
    return np.array([
        fast_count / total * 100,
        slow_count / total * 100,
        medium_count / total * 100
    ])
def calculate_pre_trade_slopes_ultra_fast(pre_candles):
    """
    Version optimis√©e de la fonction slopes pour l'ultra-rapide
    """
    try:
        durations = pre_candles['sc_candleDuration'].values
        volumes = pre_candles['sc_volume_perTick'].values
        session_starts = (pre_candles['sc_sessionStartEnd'] == 10).values

        # Appel direct √† votre fonction (pas d'optimisation possible ici)
        duration_slopes, r2_dur, std_dur = calculate_slopes_and_r2_numba(
            durations, session_starts, SLOPE_PERIODS,clip_slope=False,
                                  include_close_bar=True)
        volume_slopes, r2_vol, std_vol = calculate_slopes_and_r2_numba(volumes, session_starts, SLOPE_PERIODS,  clip_slope=False,
include_close_bar=True)

        # Retour optimis√© (acc√®s direct)
        return {
            'duration_slope': duration_slopes[-1] if len(duration_slopes) > 0 else 0,
            'duration_r2': r2_dur[-1] if len(r2_dur) > 0 else 0,
            'duration_std': std_dur[-1] if len(std_dur) > 0 else 0,
            'volume_slope': volume_slopes[-1] if len(volume_slopes) > 0 else 0,
            'volume_r2': r2_vol[-1] if len(r2_vol) > 0 else 0,
            'volume_std': std_vol[-1] if len(std_vol) > 0 else 0
        }
    except:
        return {
            'duration_slope': 0, 'duration_r2': 0, 'duration_std': 0,
            'volume_slope': 0, 'volume_r2': 0, 'volume_std': 0
        }




# VERSION ULTRA-RAPIDE DE LA FONCTION PRINCIPALE


# REMPLACEZ ENTI√àREMENT votre fonction analyze_pre_trade_conditions_ultra_fast par cette version :

# NOUVELLE FONCTION : M√©triques avec votre logique adapt√©e
def calculate_pre_trade_metrics_complete_debug_with_adaptation(pre_candles):
    """
    Version finale utilisant votre logique de slopes adapt√©e
    """
    try:
        required_cols = ['sc_candleDuration', 'sc_volume_perTick']
        if not all(col in pre_candles.columns for col in required_cols):
            return None

        durations = pre_candles['sc_candleDuration'].values
        volumes = pre_candles['sc_volume_perTick'].values

        if len(durations) == 0 or len(volumes) == 0:
            return None
        if np.all(np.isnan(durations)) or np.all(np.isnan(volumes)):
            return None

        # Calculs de base (rapides)
        stats = {
            'duration_mean': np.nanmean(durations),
            'duration_median': np.nanmedian(durations),
            'duration_std': np.nanstd(durations),
            'volume_mean': np.nanmean(volumes),
            'volume_median': np.nanmedian(volumes),
            'volume_std': np.nanstd(volumes),
            'fast_candles_pct': np.mean(durations < 10) * 100,
            'slow_candles_pct': np.mean(durations > 300) * 100,
        }

        # Coefficients de variation
        stats['duration_cv'] = stats['duration_std'] / stats['duration_mean'] if stats['duration_mean'] > 0 else 0
        stats['volume_cv'] = stats['volume_std'] / stats['volume_mean'] if stats['volume_mean'] > 0 else 0

        # SLOPES avec votre logique adapt√©e
        if len(durations) >= 3:
            # DEBUG: Afficher uniquement pour les premiers trades
            debug_mode = len(durations) == SLOPE_PERIODS  # Debug seulement si taille exacte
            slope_results = calculate_pre_trade_slopes_CLEAN(pre_candles, debug_mode)

            stats['duration_trend'] = slope_results['duration_slope']
            stats['duration_slope_stdev'] = slope_results['duration_std']
            stats['duration_slope_r2'] = slope_results['duration_r2']
            stats['volume_trend'] = slope_results['volume_slope']
            stats['volume_slope_stdev'] = slope_results['volume_std']
            stats['volume_slope_r2'] = slope_results['volume_r2']
        else:
            return None

        # M√©triques suppl√©mentaires
        stats['duration_volatility'] = stats['duration_cv']
        stats['volume_volatility'] = stats['volume_cv']

        if len(durations) >= 5:
            mid = len(durations) // 2
            stats['duration_acceleration'] = np.mean(durations[mid:]) - np.mean(durations[:mid])
            stats['volume_acceleration'] = np.mean(volumes[mid:]) - np.mean(volumes[:mid])
        else:
            stats['duration_acceleration'] = 0
            stats['volume_acceleration'] = 0

        return stats

    except Exception as e:
        if debug_mode:
            print(f"   ‚ùå Erreur calculate_pre_trade_metrics: {e}")
        return None

def analyze_pre_trade_conditions_ultra_fast(datasets_dict, n_candles_before=10):
    """
    Version ULTRA-RAPIDE de l'analyse compl√®te - CORRIG√âE
    """
    print(f"\nüöÄ ANALYSE ULTRA-RAPIDE DES {n_candles_before} BOUGIES PR√â-TRADE")
    print("=" * 80)

    results = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_complete.empty or df_filtered.empty:
            continue

        print(f"\n‚ö° DATASET: {name}")
        print("-" * 50)
        print(f"Mode ultra-rapide: {len(df_filtered):,} trades sur {len(df_complete):,} bougies...")

        # Optimisation 1: Index pr√©-calcul√© ultra-rapide
        df_complete_reset = df_complete.reset_index(drop=True)
        df_complete_sorted = df_complete_reset.sort_values(['session_id', 'date']).reset_index(drop=True)

        # Optimisation 2: Groupby optimis√©
        session_groups = df_complete_sorted.groupby('session_id', sort=False)
        session_indices = {}

        for session_id, group in session_groups:
            session_indices[session_id] = group.reset_index(drop=True)

        print(f"   üî• {len(session_indices)} sessions index√©es (ultra-rapide)")

        # Optimisation 3: Traitement ultra-vectoris√© CORRIG√â
        pre_trade_stats = []
        total_trades = len(df_filtered)
        processed = 0

        trades_by_session = df_filtered.groupby('session_id', sort=False)

        for session_id, session_trades in trades_by_session:
            if session_id not in session_indices:
                continue

            session_data = session_indices[session_id]

            if len(session_data) < n_candles_before + 1:
                continue

            # CORRECTION: Utiliser la bonne fonction avec les bons arguments
            session_stats = process_session_trades_CLEAN(
                session_trades, session_data, n_candles_before
            )

            pre_trade_stats.extend(session_stats)
            processed += len(session_trades)

            # Affichage optimis√© (moins fr√©quent)
            if processed % 1000 == 0 or processed == total_trades:
                pct = (processed / total_trades) * 100
                print(f"   ‚ö° Progression: {processed:,}/{total_trades:,} ({pct:.1f}%)")

        print(f"   ‚úÖ ULTRA-RAPIDE: {len(pre_trade_stats):,} trades analys√©s")

        if pre_trade_stats:
            # Analyse rapide des r√©sultats
            analysis = analyze_complete_stats_fast(pre_trade_stats, name)
            results[name] = analysis
            display_complete_results_fast(analysis, name)

    return results



# AJOUTEZ CETTE FONCTION COMPL√àTE D'ANALYSE DES PATTERNS :

def analyze_spring_momentum_exhaustion_patterns(analysis_results):
    """
    Analyse compl√®te des patterns Spring, Momentum et √âpuisement avec filtrage
    """
    print(f"\n" + "=" * 80)
    print("üîç ANALYSE COMPL√àTE DES PATTERNS - SPRING vs MOMENTUM vs √âPUISEMENT")
    print("=" * 80)

    # Configuration des patterns
    pattern_config = {
        'spring': {
            'name': 'Spring Pattern',
            'description': 'Volume‚Üì + Duration‚Üì (Accumulation discr√®te avant explosion)',
            'volume_condition': lambda vol_trend: vol_trend < -0.005,
            'duration_condition': lambda dur_trend: dur_trend < -0.005,
            'expected_winrate': 'High (52-54%)',
            'interpretation': 'Optimal timing - Early entry before momentum',
            'recommendation': 'üü¢ TRADE - Best conditions'
        },
        'momentum': {
            'name': 'Momentum Pattern',
            'description': 'Volume‚Üë + Duration‚Üì (Momentum with acceleration)',
            'volume_condition': lambda vol_trend: vol_trend > 0.005,
            'duration_condition': lambda dur_trend: dur_trend < -0.005,
            'expected_winrate': 'Good (51-53%)',
            'interpretation': 'Good timing - Entry during acceleration',
            'recommendation': 'üü° TRADE - Good conditions'
        },
        'spring_momentum': {
            'name': 'Spring + Momentum Combined',
            'description': 'Either Spring OR Momentum conditions',
            'volume_condition': lambda vol_trend: True,  # Will be handled in logic
            'duration_condition': lambda dur_trend: True,  # Will be handled in logic
            'expected_winrate': 'Good (51-53%)',
            'interpretation': 'Combined optimal conditions',
            'recommendation': 'üü° TRADE - Good conditions combined'
        },
        'exhaustion': {
            'name': 'Exhaustion Pattern',
            'description': 'Volume‚Üë + Duration‚Üë (Late entry - movement exhaustion)',
            'volume_condition': lambda vol_trend: vol_trend > 0.005,
            'duration_condition': lambda dur_trend: dur_trend > 0.005,
            'expected_winrate': 'Poor (49-51%)',
            'interpretation': 'Late timing - Entry on exhaustion',
            'recommendation': 'üî¥ AVOID - Poor conditions'
        },
        'all_trades': {
            'name': 'All Trades (No Filter)',
            'description': 'Original performance without any filtering',
            'volume_condition': lambda vol_trend: True,
            'duration_condition': lambda dur_trend: True,
            'expected_winrate': 'Baseline',
            'interpretation': 'Original algorithm performance',
            'recommendation': '‚ö™ BASELINE'
        }
    }

    results_summary = {}

    for dataset_name, data in analysis_results.items():
        if 'raw_data' not in data or data['raw_data'].empty:
            continue

        df = data['raw_data']
        original_trades = len(df)
        original_winrate = data['winrate']

        print(f"\nüìä DATASET: {dataset_name}")
        print("=" * 50)
        print(f"Original Performance: {original_trades:,} trades, {original_winrate:.2f}% winrate")
        print()

        dataset_results = {
            'original_trades': original_trades,
            'original_winrate': original_winrate,
            'patterns': {}
        }

        # Analyser chaque pattern
        for pattern_name, pattern_config_item in pattern_config.items():
            if pattern_name == 'spring_momentum':
                # Logique combin√©e Spring + Momentum
                spring_mask = (df['volume_trend'] < -0.005) & (df['duration_trend'] < -0.005)
                momentum_mask = (df['volume_trend'] > 0.005) & (df['duration_trend'] < -0.005)
                pattern_mask = spring_mask | momentum_mask
            elif pattern_name == 'all_trades':
                # Tous les trades
                pattern_mask = pd.Series([True] * len(df), index=df.index)
            else:
                # Patterns individuels
                vol_mask = pattern_config_item['volume_condition'](df['volume_trend'])
                dur_mask = pattern_config_item['duration_condition'](df['duration_trend'])
                pattern_mask = vol_mask & dur_mask

            # Filtrer les donn√©es
            filtered_df = df[pattern_mask]
            filtered_trades = len(filtered_df)

            if filtered_trades > 0:
                filtered_wins = (filtered_df['trade_result'] == 1).sum()
                filtered_winrate = (filtered_wins / filtered_trades) * 100
                trades_retained_pct = (filtered_trades / original_trades) * 100
                winrate_improvement = filtered_winrate - original_winrate
            else:
                filtered_winrate = 0
                trades_retained_pct = 0
                winrate_improvement = 0

            # Stocker les r√©sultats
            pattern_results = {
                'trades_count': filtered_trades,
                'winrate': filtered_winrate,
                'trades_retained_pct': trades_retained_pct,
                'winrate_improvement': winrate_improvement,
                'config': pattern_config_item
            }

            dataset_results['patterns'][pattern_name] = pattern_results

            # Affichage d√©taill√©
            if pattern_name != 'all_trades':  # Skip baseline in detailed display
                status_emoji = "üü¢" if winrate_improvement > 1 else "üü°" if winrate_improvement > -0.5 else "üî¥"
                print(f"{status_emoji} {pattern_config_item['name']:25}")
                print(f"   Trades: {filtered_trades:6,} ({trades_retained_pct:5.1f}% retained)")
                print(f"   Winrate: {filtered_winrate:5.2f}% ({winrate_improvement:+5.2f}% vs original)")
                print(f"   {pattern_config_item['recommendation']}")
                print()

        results_summary[dataset_name] = dataset_results

    # TABLEAU R√âCAPITULATIF GLOBAL
    print(f"\nüìà TABLEAU R√âCAPITULATIF - PERFORMANCE PAR PATTERN")
    print("=" * 120)

    # En-t√™te du tableau
    header = f"{'Dataset':<12} {'Pattern':<20} {'Trades':<8} {'%Kept':<6} {'WR%':<6} {'Œî WR':<6} {'Status':<15} {'Recommendation':<20}"
    print(header)
    print("-" * 120)

    # Donn√©es pour chaque dataset et pattern
    for dataset_name, dataset_data in results_summary.items():
        first_row = True

        # Ordre des patterns pour affichage
        pattern_order = ['all_trades', 'spring', 'momentum', 'spring_momentum', 'exhaustion']

        for pattern_name in pattern_order:
            if pattern_name in dataset_data['patterns']:
                pattern_data = dataset_data['patterns'][pattern_name]

                # Nom du dataset seulement sur la premi√®re ligne
                dataset_display = dataset_name if first_row else ""
                first_row = False

                # Formatage des donn√©es
                trades_display = f"{pattern_data['trades_count']:,}"
                retained_display = f"{pattern_data['trades_retained_pct']:.1f}%"
                winrate_display = f"{pattern_data['winrate']:.2f}"
                improvement_display = f"{pattern_data['winrate_improvement']:+.2f}"

                # Status bas√© sur l'am√©lioration
                if pattern_name == 'all_trades':
                    status = "BASELINE"
                elif pattern_data['winrate_improvement'] > 2:
                    status = "EXCELLENT"
                elif pattern_data['winrate_improvement'] > 1:
                    status = "VERY GOOD"
                elif pattern_data['winrate_improvement'] > 0:
                    status = "GOOD"
                elif pattern_data['winrate_improvement'] > -1:
                    status = "NEUTRAL"
                else:
                    status = "POOR"

                # Recommandation simplifi√©e
                if pattern_name == 'all_trades':
                    recommendation = "BASELINE"
                elif pattern_data['winrate_improvement'] > 1:
                    recommendation = "üü¢ TRADE"
                elif pattern_data['winrate_improvement'] > -0.5:
                    recommendation = "üü° CONSIDER"
                else:
                    recommendation = "üî¥ AVOID"

                # Pattern name display
                pattern_display = pattern_data['config']['name']

                print(
                    f"{dataset_display:<12} {pattern_display:<20} {trades_display:<8} {retained_display:<6} {winrate_display:<6} {improvement_display:<6} {status:<15} {recommendation:<20}")

        print("-" * 120)  # Separator between datasets

    # ANALYSE CROSS-DATASET DES PATTERNS
    print(f"\nüîç ANALYSE CROSS-DATASET DES PATTERNS")
    print("=" * 80)

    pattern_summary = {}
    for pattern_name in ['spring', 'momentum', 'spring_momentum', 'exhaustion']:
        pattern_performances = []
        pattern_retentions = []

        for dataset_name, dataset_data in results_summary.items():
            if pattern_name in dataset_data['patterns']:
                pattern_data = dataset_data['patterns'][pattern_name]
                pattern_performances.append(pattern_data['winrate_improvement'])
                pattern_retentions.append(pattern_data['trades_retained_pct'])

        if pattern_performances:
            avg_improvement = sum(pattern_performances) / len(pattern_performances)
            avg_retention = sum(pattern_retentions) / len(pattern_retentions)

            pattern_summary[pattern_name] = {
                'avg_improvement': avg_improvement,
                'avg_retention': avg_retention,
                'datasets_count': len(pattern_performances)
            }

    print(f"{'Pattern':<20} {'Avg ŒîWR':<8} {'Avg %Kept':<10} {'Overall Rating':<15}")
    print("-" * 60)

    # Trier par am√©lioration moyenne
    sorted_patterns = sorted(pattern_summary.items(), key=lambda x: x[1]['avg_improvement'], reverse=True)

    for pattern_name, summary in sorted_patterns:
        config = pattern_config[pattern_name]

        # Rating global
        if summary['avg_improvement'] > 2:
            rating = "üü¢ EXCELLENT"
        elif summary['avg_improvement'] > 1:
            rating = "üü¢ VERY GOOD"
        elif summary['avg_improvement'] > 0:
            rating = "üü° GOOD"
        elif summary['avg_improvement'] > -1:
            rating = "üü° NEUTRAL"
        else:
            rating = "üî¥ POOR"

        print(
            f"{config['name']:<20} {summary['avg_improvement']:+6.2f}% {summary['avg_retention']:7.1f}%   {rating:<15}")

    # RECOMMANDATIONS FINALES
    print(f"\nüí° RECOMMANDATIONS STRAT√âGIQUES FINALES")
    print("=" * 80)

    best_pattern = sorted_patterns[0]
    worst_pattern = sorted_patterns[-1]

    print(f"üèÜ MEILLEUR PATTERN: {pattern_config[best_pattern[0]]['name']}")
    print(f"   Am√©lioration moyenne: {best_pattern[1]['avg_improvement']:+.2f}%")
    print(f"   R√©tention moyenne: {best_pattern[1]['avg_retention']:.1f}%")
    print(f"   {pattern_config[best_pattern[0]]['recommendation']}")

    print(f"\nüíÄ PIRE PATTERN: {pattern_config[worst_pattern[0]]['name']}")
    print(f"   Am√©lioration moyenne: {worst_pattern[1]['avg_improvement']:+.2f}%")
    print(f"   R√©tention moyenne: {worst_pattern[1]['avg_retention']:.1f}%")
    print(f"   {pattern_config[worst_pattern[0]]['recommendation']}")

    # Recommandations par dataset
    print(f"\nüìä RECOMMANDATIONS PAR DATASET:")
    for dataset_name, dataset_data in results_summary.items():
        best_pattern_for_dataset = None
        best_improvement = -999

        for pattern_name, pattern_data in dataset_data['patterns'].items():
            if pattern_name != 'all_trades' and pattern_data['winrate_improvement'] > best_improvement:
                best_improvement = pattern_data['winrate_improvement']
                best_pattern_for_dataset = pattern_name

        if best_pattern_for_dataset:
            config = pattern_config[best_pattern_for_dataset]
            pattern_data = dataset_data['patterns'][best_pattern_for_dataset]

            print(f"\nüéØ {dataset_name}:")
            print(f"   Meilleur pattern: {config['name']}")
            print(
                f"   Impact: {pattern_data['winrate_improvement']:+.2f}% winrate, {pattern_data['trades_retained_pct']:.1f}% trades kept")

            if pattern_data['winrate_improvement'] > 1:
                print(f"   ‚úÖ RECOMMANDATION: Impl√©menter ce filtre")
            elif pattern_data['winrate_improvement'] > 0:
                print(f"   üü° RECOMMANDATION: Tester en condition r√©elle")
            else:
                print(f"   ‚ùå RECOMMANDATION: Garder la configuration actuelle")

    return results_summary


# AJOUTEZ CETTE FONCTION DANS LA FONCTION main() APR√àS add_missing_slopes_analysis :
# ===== SUPPRESSION DES DOUBLONS =====
# Gardez seulement UNE SEULE version de fast_candle_classification

@njit
def fast_candle_classification(durations):
    """Classification rapide des types de bougies"""
    if len(durations) == 0:
        return np.array([0.0, 0.0, 0.0])

    fast_count = np.sum(durations < 10)
    slow_count = np.sum(durations > 300)
    medium_count = len(durations) - fast_count - slow_count

    total = len(durations)
    return np.array([
        fast_count / total * 100,
        slow_count / total * 100,
        medium_count / total * 100
    ])


# ===== VERSION SANS FALLBACK AVEC AFFICHAGE SLOPES =====

def calculate_pre_trade_slopes_DISPLAY(pre_candles, debug_mode=True):
    """
    Version qui AFFICHE les slopes calcul√©es par calculate_slopes_and_r2_numba
    SANS AUCUN FALLBACK
    """
    try:
        durations = pre_candles['sc_candleDuration'].values
        volumes = pre_candles['sc_volume_perTick'].values

        if len(durations) < 3 or len(volumes) < 3:
            if debug_mode:
                print(f"   ‚ùå Pas assez de donn√©es: dur√©es={len(durations)}, volumes={len(volumes)}")
            return {
                'duration_slope': 0, 'duration_r2': 0, 'duration_std': 0,
                'volume_slope': 0, 'volume_r2': 0, 'volume_std': 0
            }

        # Cr√©er session_starts
        session_starts_dur = np.zeros(len(durations), dtype=bool)
        session_starts_vol = np.zeros(len(volumes), dtype=bool)
        session_starts_dur[0] = True
        session_starts_vol[0] = True

        if debug_mode:
            print(f"   üîß INPUT pour calculate_slopes_and_r2_numba:")
            print(f"      Dur√©es: {len(durations)} valeurs [{np.min(durations):.1f}, {np.max(durations):.1f}]")
            print(f"      Volumes: {len(volumes)} valeurs [{np.min(volumes):.1f}, {np.max(volumes):.1f}]")
            print(f"      SLOPE_PERIODS: {SLOPE_PERIODS}")
            print(f"      clip_slope: False")

        # ‚úÖ APPEL DIRECT √Ä VOTRE FONCTION - AUCUN FALLBACK
        duration_slopes, r2_dur, std_dur = calculate_slopes_and_r2_numba(
            durations, session_starts_dur, SLOPE_PERIODS,
            clip_slope=False,  # ‚Üê SANS CLIPPING
            include_close_bar=True
        )

        volume_slopes, r2_vol, std_vol = calculate_slopes_and_r2_numba(
            volumes, session_starts_vol, SLOPE_PERIODS,
            clip_slope=False,  # ‚Üê SANS CLIPPING
            include_close_bar=True
        )

        if debug_mode:
            print(f"   üìä R√âSULTATS calculate_slopes_and_r2_numba:")
            print(f"      Duration slopes: array de {len(duration_slopes)} valeurs")
            if len(duration_slopes) > 0:
                print(f"         Premi√®re: {duration_slopes[0]:.6f}")
                print(f"         Derni√®re: {duration_slopes[-1]:.6f}")
                print(f"         NaN count: {np.sum(np.isnan(duration_slopes))}")

            print(f"      Volume slopes: array de {len(volume_slopes)} valeurs")
            if len(volume_slopes) > 0:
                print(f"         Premi√®re: {volume_slopes[0]:.6f}")
                print(f"         Derni√®re: {volume_slopes[-1]:.6f}")
                print(f"         NaN count: {np.sum(np.isnan(volume_slopes))}")

        # Extraction des derni√®res valeurs valides
        dur_slope = 0
        vol_slope = 0
        dur_r2 = 0
        vol_r2 = 0
        dur_std = 0
        vol_std = 0

        if len(duration_slopes) > 0:
            valid_dur_mask = ~np.isnan(duration_slopes)
            if np.any(valid_dur_mask):
                dur_slope = duration_slopes[valid_dur_mask][-1]
                dur_r2 = r2_dur[valid_dur_mask][-1] if len(r2_dur) > 0 else 0
                dur_std = std_dur[valid_dur_mask][-1] if len(std_dur) > 0 else 0

        if len(volume_slopes) > 0:
            valid_vol_mask = ~np.isnan(volume_slopes)
            if np.any(valid_vol_mask):
                vol_slope = volume_slopes[valid_vol_mask][-1]
                vol_r2 = r2_vol[valid_vol_mask][-1] if len(r2_vol) > 0 else 0
                vol_std = std_vol[valid_vol_mask][-1] if len(std_vol) > 0 else 0

        if debug_mode:
            print(f"   ‚úÖ SLOPES FINALES EXTRAITES:")
            print(f"      Duration slope: {dur_slope:.6f} (R¬≤={dur_r2:.3f}, Std={dur_std:.3f})")
            print(f"      Volume slope: {vol_slope:.6f} (R¬≤={vol_r2:.3f}, Std={vol_std:.3f})")

        return {
            'duration_slope': dur_slope,
            'duration_r2': dur_r2,
            'duration_std': dur_std,
            'volume_slope': vol_slope,
            'volume_r2': vol_r2,
            'volume_std': vol_std
        }

    except Exception as e:
        if debug_mode:
            print(f"   ‚ùå ERREUR FATALE calculate_slopes_and_r2_numba: {e}")
            print(f"   ‚ùå AUCUN FALLBACK - RETOUR DE Z√âROS")

        # ‚ùå AUCUN FALLBACK - RETOUR Z√âROS SEULEMENT
        return {
            'duration_slope': 0, 'duration_r2': 0, 'duration_std': 0,
            'volume_slope': 0, 'volume_r2': 0, 'volume_std': 0
        }


# Strat√©gie de filtrage pour am√©liorer TEST sans d√©grader les autres datasets
# Strat√©gie de filtrage pour am√©liorer TEST sans d√©grader les autres datasets

def apply_anti_spring_universal_filter(
        df,
        volume_slope_col='volume_slope',
        duration_slope_col='duration_slope'):

    # --- auto-d√©tection : si le nom par d√©faut est absent,
    #     on bascule sur la nouvelle convention -------------
    if volume_slope_col not in df.columns and 'volume_trend' in df.columns:
        volume_slope_col = 'volume_trend'
    if duration_slope_col not in df.columns and 'duration_trend' in df.columns:
        duration_slope_col = 'duration_trend'

    # si les colonnes manquent toujours ‚Üí on ne filtre pas
    if (volume_slope_col not in df.columns) or (duration_slope_col not in df.columns):
        print("‚ö†Ô∏è  Colonnes slope introuvables ‚Äì filtre ignor√©")
        return df.copy()

    # -------- filtre Anti-Spring --------
    spring_mask      = (df[volume_slope_col] < 0) & (df[duration_slope_col] < 0)
    anti_spring_mask = ~spring_mask
    return df[anti_spring_mask]

def analyze_anti_spring_impact(datasets, label_col='class_binaire'):
    """
    datasets :  ‚Ä¢ dict  {name: df}
                ‚Ä¢ ou liste de tuples (df, name)   **ou**   (name, df)
    """
    import pandas as pd
    results = {}

    # 1) normaliser l‚Äôit√©ration
    items = datasets.items() if isinstance(datasets, dict) else datasets

    for pair in items:
        # ‚Äî d√©baller intelligemment ‚Äî
        if isinstance(pair, tuple) and len(pair) == 2:
            # Cas (df, name)  ou  (name, df)
            if isinstance(pair[0], pd.DataFrame):
                df, dataset_name = pair            # (df, name)
            else:
                dataset_name, df = pair            # (name, df)
        else:
            raise ValueError("Chaque √©l√©ment doit √™tre un tuple (df, name) ou (name, df)")

        # 2) filtrer 0 / 1
        valid_mask = df[label_col].isin([0, 1])
        df_valid   = df[valid_mask]

        orig_trades = len(df_valid)
        orig_wr     = df_valid[label_col].mean() if orig_trades else 0

        # 3) appliquer le filtre anti-Spring
        df_filtered = apply_anti_spring_universal_filter(df_valid)
        filt_trades = len(df_filtered)
        filt_wr     = df_filtered[label_col].mean() if filt_trades else 0

        results[dataset_name] = {
            'original_trades':  orig_trades,
            'filtered_trades':  filt_trades,
            'retention_rate':   filt_trades / orig_trades * 100 if orig_trades else 0,
            'original_winrate': orig_wr,
            'filtered_winrate': filt_wr,
            'winrate_improvement': filt_wr - orig_wr
        }

        print(f"üìä {dataset_name}:"
              f"  Trades {orig_trades:,} ‚Üí {filt_trades:,}"
              f"  ({results[dataset_name]['retention_rate']:.1f}% retenus) |"
              f"  WR {orig_wr:5.2%} ‚Üí {filt_wr:5.2%}"
              f"  ({results[dataset_name]['winrate_improvement']:+.2%})")

    return results


def adaptive_pattern_filter(df, dataset_type,
                            volume_slope_col='volume_slope',
                            duration_slope_col='duration_slope'):

    # --- si les slopes n‚Äôexistent pas on ne filtre pas ---
    if volume_slope_col not in df.columns or duration_slope_col not in df.columns:
        print(f"‚ö†Ô∏è  {dataset_type}: colonnes de slope absentes ‚Üí aucun filtre appliqu√©")
        exit(45)
        return df.copy()

    # (logique inchang√©e ensuite)
    if dataset_type == "TEST":
        condition = ~((df[volume_slope_col] < 0) & (df[duration_slope_col] < 0))
    elif dataset_type == "TRAIN":
        condition = df[volume_slope_col] > 0
    elif dataset_type == "VALIDATION":
        condition = ~((df[volume_slope_col] > 0) & (df[duration_slope_col] < -0.5))
    else:  # VALIDATION 1, UNSEEN, etc.
        condition = ~((df[volume_slope_col] < 0) & (df[duration_slope_col] < 0)) \
                    & (df[volume_slope_col] > -0.1)

    filtered_df = df[condition]
    print(f"üéØ {dataset_type}: {len(filtered_df)}/{len(df)} trades conserv√©s "
          f"({len(filtered_df)/len(df)*100:.1f} %)")
    return filtered_df



# Fonction principale pour tester la strat√©gie
def test_universal_anti_spring_strategy(datasets_dict,
                                        label_col='class_binaire'):
    """
    Test la strat√©gie anti-Spring universelle + filtre adaptatif
    """
    print("üîç TEST STRAT√âGIE ANTI-SPRING UNIVERSELLE")
    print("=" * 60)

    # --- 1) normaliser l‚Äôit√©ration -----------------------------
    if isinstance(datasets_dict, dict):
        items = datasets_dict.items()          # {'TRAIN': df, ...}
    else:                                      # [(df, name) ou (name, df)]
        items = [(name, df) if isinstance(pair[0], str) else (pair[1], pair[0])
                 for pair in datasets_dict]

    # --- 2) strat√©gie 1 : anti-Spring universel ----------------
    print("\nüìä STRAT√âGIE 1 : Anti-Spring universel")
    universal_results = analyze_anti_spring_impact(datasets_dict,
                                                   label_col=label_col)

    # --- 3) strat√©gie 2 : filtre adaptatif ---------------------
    print("\nüìä STRAT√âGIE 2 : Filtre adaptatif par dataset")
    adaptive_results = {}

    for dataset_name, df in items:
        if label_col not in df.columns:
            raise KeyError(f"Colonne '{label_col}' absente de {dataset_name}")

        original_wr   = df[label_col].mean()
        filtered_df   = adaptive_pattern_filter(df, dataset_name)
        filtered_wr   = filtered_df[label_col].mean() if len(filtered_df) else 0

        adaptive_results[dataset_name] = {
            'original_winrate' : original_wr,
            'filtered_winrate' : filtered_wr,
            'improvement'      : filtered_wr - original_wr
        }

    # --- 4) comparaison ---------------------------------------
    print("\nüìà COMPARAISON DES STRAT√âGIES")
    print("=" * 60)
    for dataset_name in adaptive_results:
        univ_imp  = universal_results[dataset_name]['winrate_improvement']
        adapt_imp = adaptive_results[dataset_name]['improvement']
        better    = "Universelle" if univ_imp > adapt_imp else "Adaptative"

        print(f"{dataset_name}:  anti-Spring {univ_imp:+.2%} | "
              f"adaptatif {adapt_imp:+.2%}  ‚Üí  üèÜ {better}")

    return universal_results, adaptive_results




def calculate_pre_trade_metrics_DISPLAY(pre_candles):
    """
    Version qui AFFICHE les m√©triques et utilise calculate_slopes_and_r2_numba
    SANS FALLBACK
    """
    try:
        required_cols = ['sc_candleDuration', 'sc_volume_perTick']
        if not all(col in pre_candles.columns for col in required_cols):
            print(f"   ‚ùå Colonnes manquantes: {[col for col in required_cols if col not in pre_candles.columns]}")
            return None

        durations = pre_candles['sc_candleDuration'].values
        volumes = pre_candles['sc_volume_perTick'].values

        if len(durations) == 0 or len(volumes) == 0:
            print(f"   ‚ùå Arrays vides: dur={len(durations)}, vol={len(volumes)}")
            return None
        if np.all(np.isnan(durations)) or np.all(np.isnan(volumes)):
            print(f"   ‚ùå Toutes les valeurs sont NaN")
            return None

        print(f"   üìä Calcul m√©triques pr√©-trade:")
        print(f"      Input: {len(durations)} dur√©es, {len(volumes)} volumes")

        # Calculs de base
        stats = {
            'duration_mean': np.nanmean(durations),
            'duration_median': np.nanmedian(durations),
            'duration_std': np.nanstd(durations),
            'volume_mean': np.nanmean(volumes),
            'volume_median': np.nanmedian(volumes),
            'volume_std': np.nanstd(volumes),
            'fast_candles_pct': np.mean(durations < 10) * 100,
            'slow_candles_pct': np.mean(durations > 300) * 100,
        }

        # Coefficients de variation
        stats['duration_cv'] = stats['duration_std'] / stats['duration_mean'] if stats['duration_mean'] > 0 else 0
        stats['volume_cv'] = stats['volume_std'] / stats['volume_mean'] if stats['volume_mean'] > 0 else 0

        print(f"      Stats de base calcul√©es ‚úì")

        # ‚úÖ SLOPES AVEC AFFICHAGE - SANS FALLBACK
        if len(durations) >= 3:
            print(f"   üîß Calcul des slopes avec votre fonction:")
            slope_results = calculate_pre_trade_slopes_DISPLAY(pre_candles, debug_mode=True)

            stats['duration_trend'] = slope_results['duration_slope']
            stats['duration_slope_stdev'] = slope_results['duration_std']
            stats['duration_slope_r2'] = slope_results['duration_r2']
            stats['volume_trend'] = slope_results['volume_slope']
            stats['volume_slope_stdev'] = slope_results['volume_std']
            stats['volume_slope_r2'] = slope_results['volume_r2']

            print(f"   ‚úÖ Slopes int√©gr√©es dans stats")
        else:
            print(f"   ‚ùå Pas assez de donn√©es pour les slopes ({len(durations)} < 3)")
            return None

        # M√©triques suppl√©mentaires
        stats['duration_volatility'] = stats['duration_cv']
        stats['volume_volatility'] = stats['volume_cv']

        if len(durations) >= 5:
            mid = len(durations) // 2
            stats['duration_acceleration'] = np.mean(durations[mid:]) - np.mean(durations[:mid])
            stats['volume_acceleration'] = np.mean(volumes[mid:]) - np.mean(volumes[:mid])
        else:
            stats['duration_acceleration'] = 0
            stats['volume_acceleration'] = 0

        print(f"   ‚úÖ M√©triques compl√®tes calcul√©es")
        return stats

    except Exception as e:
        print(f"   ‚ùå ERREUR calculate_pre_trade_metrics_DISPLAY: {e}")
        return None


def process_session_trades_DISPLAY(session_trades, session_data, n_candles_before):
    """
    Version avec affichage pour le traitement de session
    """
    session_stats = []
    print(f"   üìä Traitement session: {len(session_trades)} trades")

    session_dates_np = session_data['date'].values.astype('datetime64[ns]')

    for i, (_, trade_row) in enumerate(session_trades.iterrows()):
        if i == 0:  # Afficher seulement pour le premier trade
            print(f"   üîç Exemple trade #{i + 1}:")

        trade_date_np = np.datetime64(trade_row['date'])

        mask = session_dates_np <= trade_date_np
        indices = np.where(mask)[0]

        if len(indices) < n_candles_before + 1:
            if i == 0:
                print(f"      ‚ùå Pas assez de bougies: {len(indices)} < {n_candles_before + 1}")
            continue

        trade_position = indices[-1]

        if trade_position < n_candles_before:
            if i == 0:
                print(f"      ‚ùå Position trop proche du d√©but: {trade_position} < {n_candles_before}")
            continue

        start_pos = trade_position - n_candles_before
        end_pos = trade_position

        pre_candles = session_data.iloc[start_pos:end_pos]

        if len(pre_candles) == n_candles_before:
            if i == 0:
                print(f"      ‚úÖ Extraction pr√©-trade: {len(pre_candles)} bougies")
                print(f"         P√©riode: {pre_candles['date'].iloc[0]} √† {pre_candles['date'].iloc[-1]}")

            # ‚úÖ UTILISE LA VERSION AVEC AFFICHAGE
            stats = calculate_pre_trade_metrics_DISPLAY(pre_candles) if i == 0 else calculate_pre_trade_metrics_CLEAN(
                pre_candles)

            if stats:
                stats.update({
                    'trade_result': trade_row['class_binaire'],
                    'session_id': trade_row['session_id'],
                    'trade_date': trade_row['date']
                })
                session_stats.append(stats)

                if i == 0:
                    print(f"      ‚úÖ Stats ajout√©es pour trade #{i + 1}")

    print(f"   ‚úÖ Session termin√©e: {len(session_stats)} trades analys√©s")
    return session_stats


def analyze_pre_trade_conditions_DISPLAY(datasets_dict, n_candles_before=10):
    """
    Version AVEC AFFICHAGE des slopes - SANS FALLBACK
    """
    print(f"\nüîß ANALYSE PR√â-TRADE AVEC AFFICHAGE DES SLOPES")
    print("=" * 80)

    results = {}

    for name, (df_complete, df_filtered) in datasets_dict.items():
        if df_complete.empty or df_filtered.empty:
            continue

        print(f"\nüîß DATASET: {name}")
        print("-" * 50)

        # Pr√©paration optimis√©e
        df_complete_sorted = df_complete.sort_values(['session_id', 'date']).reset_index(drop=True)
        session_groups = df_complete_sorted.groupby('session_id', sort=False)
        session_indices = {session_id: group.reset_index(drop=True)
                           for session_id, group in session_groups}

        pre_trade_stats = []
        total_trades = len(df_filtered)
        processed = 0

        trades_by_session = df_filtered.groupby('session_id', sort=False)

        # TRAITER SEULEMENT LA PREMI√àRE SESSION POUR AFFICHAGE
        first_session = True

        for session_id, session_trades in trades_by_session:
            if session_id not in session_indices:
                continue

            session_data = session_indices[session_id]
            if len(session_data) < n_candles_before + 1:
                continue

            if first_session:
                print(f"\nüîç AFFICHAGE D√âTAILL√â SESSION #{session_id}:")
                session_stats = process_session_trades_DISPLAY(
                    session_trades, session_data, n_candles_before
                )
                first_session = False
            else:
                # Sessions suivantes sans affichage
                session_stats = process_session_trades_CLEAN(
                    session_trades, session_data, n_candles_before
                )

            pre_trade_stats.extend(session_stats)
            processed += len(session_trades)

            if processed % 1000 == 0:
                pct = (processed / total_trades) * 100
                print(f"   Progression: {processed:,}/{total_trades:,} ({pct:.1f}%)")

        print(f"   ‚úÖ TOTAL: {len(pre_trade_stats):,} trades analys√©s pour {name}")

        if pre_trade_stats:
            analysis = analyze_complete_stats_fast(pre_trade_stats, name)
            results[name] = analysis
            display_complete_results_fast(analysis, name)

    return results


# ===== FONCTION MODIFI√âE DANS MAIN() =====

def add_ultra_fast_pre_trade_analysis_DISPLAY(datasets_dict, n_candles_before=10):
    """
    Version avec affichage des slopes pour debugging
    """
    print(f"\nüîß ANALYSE AVEC AFFICHAGE DES SLOPES")
    print(f"üîß Utilisation de calculate_slopes_and_r2_numba SANS fallback")

    results = analyze_pre_trade_conditions_DISPLAY(datasets_dict, n_candles_before)

    print(f"\n‚úÖ ANALYSE AVEC AFFICHAGE TERMIN√âE")
    return results


# ===== REMPLACEZ DANS VOTRE MAIN() =====
"""
# Remplacez cette ligne:
pre_trade_results = add_ultra_fast_pre_trade_analysis(datasets_dict, n_candles_before=SLOPE_PERIODS)

# Par cette ligne:
pre_trade_results = add_ultra_fast_pre_trade_analysis_DISPLAY(datasets_dict, n_candles_before=SLOPE_PERIODS)
"""
def add_pattern_analysis_to_main(pre_trade_results):
    """
    Ajoute l'analyse des patterns Spring/Momentum/√âpuisement au script principal
    """
    pattern_results = analyze_spring_momentum_exhaustion_patterns(pre_trade_results)
    return pattern_results


# DANS VOTRE FONCTION main(), AJOUTEZ CETTE LIGNE APR√àS slopes_analysis :

# slopes_analysis = add_missing_slopes_analysis(pre_trade_results)
#
# # NOUVELLE LIGNE √Ä AJOUTER :
# pattern_analysis = add_pattern_analysis_to_main(pre_trade_results)

# FONCTION BONUS : Configuration des seuils de patterns
def configure_pattern_thresholds():
    """
    Configuration des seuils pour les patterns - adapt√©e aux pentes clipp√©es [-1, +1]
    """
    return {
        'spring_volume_threshold': -5,  # Volume baisse forte (20%)
        'spring_duration_threshold': 5,  # Acc√©l√©ration forte (20%)
        'momentum_volume_threshold': 5,  # Volume monte mod√©r√© (10%)
        'momentum_duration_threshold': -5,  # Acc√©l√©ration mod√©r√©e (15%)
        'exhaustion_volume_threshold': 5,  # Volume monte fort (20%)
        'exhaustion_duration_threshold': 5  # Ralentissement mod√©r√© (10%)
    }


# ===== CORRECTION 1: Fonction pour merger les r√©sultats slopes avec les datasets originaux =====

def merge_slopes_with_datasets(pre_trade_results, datasets_dict):
    """
    Merge les slopes calcul√©es dans pre_trade_results avec les datasets originaux
    """
    merged_datasets = {}

    for dataset_name, (df_complete, df_filtered) in datasets_dict.items():
        if dataset_name in pre_trade_results and 'raw_data' in pre_trade_results[dataset_name]:
            slopes_df = pre_trade_results[dataset_name]['raw_data']

            print(f"üìä Merging slopes pour {dataset_name}:")
            print(f"   Original filtered: {len(df_filtered):,} trades")
            print(f"   Slopes calcul√©es: {len(slopes_df):,} trades")

            # Merger sur date et session_id
            df_merged = df_filtered.merge(
                slopes_df[['trade_date', 'session_id', 'volume_trend', 'duration_trend', 'trade_result']],
                left_on=['date', 'session_id'],
                right_on=['trade_date', 'session_id'],
                how='inner'
            )

            print(f"   Apr√®s merge: {len(df_merged):,} trades avec slopes")

            # V√©rifier les colonnes slopes
            if 'volume_trend' in df_merged.columns and 'duration_trend' in df_merged.columns:
                print(f"   ‚úÖ Colonnes slopes pr√©sentes")
                print(
                    f"   üìä Volume trend range: [{df_merged['volume_trend'].min():.4f}, {df_merged['volume_trend'].max():.4f}]")
                print(
                    f"   üìä Duration trend range: [{df_merged['duration_trend'].min():.4f}, {df_merged['duration_trend'].max():.4f}]")
            else:
                print(f"   ‚ùå Colonnes slopes manquantes")

            merged_datasets[dataset_name] = df_merged
        else:
            print(f"‚ùå {dataset_name}: Pas de r√©sultats slopes disponibles")
            merged_datasets[dataset_name] = df_filtered

    return merged_datasets


# ===== CORRECTION 2: Fonction apply_anti_spring_universal_filter corrig√©e =====

def apply_anti_spring_universal_filter_fixed(df,
                                             volume_slope_col='volume_trend',
                                             duration_slope_col='duration_trend'):
    """
    Version corrig√©e qui v√©rifie d'abord la pr√©sence des colonnes
    """
    print(f"   üîç Colonnes disponibles: {list(df.columns)[:10]}...")  # Afficher les 10 premi√®res colonnes

    # V√©rifier la pr√©sence des colonnes slopes
    if volume_slope_col not in df.columns:
        print(f"   ‚ùå Colonne '{volume_slope_col}' manquante")
        return df.copy()

    if duration_slope_col not in df.columns:
        print(f"   ‚ùå Colonne '{duration_slope_col}' manquante")
        return df.copy()

    print(f"   ‚úÖ Colonnes slopes trouv√©es: {volume_slope_col}, {duration_slope_col}")

    # Statistiques avant filtrage
    original_count = len(df)
    original_winrate = df['class_binaire'].mean() if 'class_binaire' in df.columns else 0

    # D√©finir le pattern Spring: Volume‚Üì ET Duration‚Üì
    spring_mask = (df[volume_slope_col] < 1.5) & (df[duration_slope_col] <1.5) #pour les < et > semble invers√©
    spring_count = ~spring_mask.sum()

    print(
        f"   üìä Spring pattern d√©tect√©: {spring_count:,}/{original_count:,} trades ({spring_count / original_count * 100:.1f}%)")

    # Appliquer le filtre Anti-Spring
    anti_spring_mask = ~spring_mask

    filtered_df = df[anti_spring_mask]

    filtered_count = len(filtered_df)
    filtered_winrate = filtered_df['class_binaire'].mean() if 'class_binaire' in filtered_df.columns else 0

    print(f"   üìä Apr√®s filtre: {filtered_count:,} trades retenus ({filtered_count / original_count * 100:.1f}%)")
    print(f"   üìä Winrate: {original_winrate:.4f} ‚Üí {filtered_winrate:.4f} ({filtered_winrate - original_winrate:+.4f})")

    return filtered_df


# ===== CORRECTION 3: Fonction analyze_anti_spring_impact corrig√©e =====

def analyze_anti_spring_impact_fixed(datasets, label_col='class_binaire'):
    """
    Version corrig√©e qui utilise les datasets avec slopes
    """
    results = {}

    # Normaliser l'it√©ration
    if isinstance(datasets, dict):
        items = datasets.items()
    else:
        items = datasets

    for dataset_name, df in items:
        print(f"\nüìä Analyse Anti-Spring: {dataset_name}")
        print("-" * 50)

        # Filtrer les trades valides (0/1)
        if label_col not in df.columns:
            print(f"   ‚ùå Colonne '{label_col}' manquante dans {dataset_name}")
            continue

        valid_mask = df[label_col].isin([0, 1])
        df_valid = df[valid_mask]

        orig_trades = len(df_valid)
        orig_wr = df_valid[label_col].mean() if orig_trades > 0 else 0

        print(f"   üìä Donn√©es valides: {orig_trades:,} trades")

        # Appliquer le filtre anti-Spring
        df_filtered = apply_anti_spring_universal_filter_fixed(df_valid)

        filt_trades = len(df_filtered)
        filt_wr = df_filtered[label_col].mean() if filt_trades > 0 else 0

        retention_rate = filt_trades / orig_trades * 100 if orig_trades > 0 else 0
        winrate_improvement = filt_wr - orig_wr

        results[dataset_name] = {
            'original_trades': orig_trades,
            'filtered_trades': filt_trades,
            'retention_rate': retention_rate,
            'original_winrate': orig_wr,
            'filtered_winrate': filt_wr,
            'winrate_improvement': winrate_improvement
        }

        print(f"   ‚úÖ R√âSULTAT: {orig_trades:,} ‚Üí {filt_trades:,} trades ({retention_rate:.1f}% retenus)")
        print(f"   ‚úÖ WINRATE: {orig_wr:.4f} ‚Üí {filt_wr:.4f} ({winrate_improvement:+.4f})")

    return results

def test_universal_anti_spring_strategy_fixed(datasets_dict, pre_trade_results, label_col='class_binaire'):
    """
    Version corrig√©e qui utilise les r√©sultats slopes
    """
    print("üîç TEST STRAT√âGIE ANTI-SPRING UNIVERSELLE (VERSION CORRIG√âE)")
    print("=" * 80)

    # √âtape 1: Merger les slopes avec les datasets originaux
    print("\nüìä √âTAPE 1: Merge des slopes avec les datasets")
    print("-" * 60)

    merged_datasets = merge_slopes_with_datasets(pre_trade_results, datasets_dict)

    # √âtape 2: Analyser l'impact du filtre anti-Spring
    print("\nüìä √âTAPE 2: Application du filtre anti-Spring")
    print("-" * 60)

    universal_results = analyze_anti_spring_impact_fixed(merged_datasets, label_col=label_col)

    # √âtape 3: Afficher le r√©sum√©
    print("\nüìà R√âSUM√â DE L'IMPACT ANTI-SPRING")
    print("=" * 70)

    print(f"{'Dataset':<15} {'Original':<8} {'Filtr√©':<8} {'R√©tention':<10} {'WR Orig':<8} {'WR Filt':<8} {'Œî WR':<8}")
    print("-" * 70)

    for dataset_name, result in universal_results.items():
        print(f"{dataset_name:<15} "
              f"{result['original_trades']:>7,} "
              f"{result['filtered_trades']:>7,} "
              f"{result['retention_rate']:>8.1f}% "
              f"{result['original_winrate']:>7.2%} "
              f"{result['filtered_winrate']:>7.2%} "
              f"{result['winrate_improvement']:>+7.2%}")

    # √âtape 4: Analyse sp√©cifique TEST
    if 'TEST' in universal_results:
        test_result = universal_results['TEST']
        print(f"\nüéØ FOCUS TEST:")
        print(f"   Am√©lioration winrate: {test_result['winrate_improvement']:+.2%}")

        if test_result['winrate_improvement'] > 0.01:  # +1%
            print(f"   ‚úÖ SUCC√àS: Le filtre anti-Spring am√©liore TEST")
        elif test_result['winrate_improvement'] > 0:
            print(f"   üü° MOD√âR√â: L√©g√®re am√©lioration")
        else:
            print(f"   ‚ùå √âCHEC: Pas d'am√©lioration")

    return universal_results


def main():
    """
    Fonction principale
    """
    global SLOPE_PERIODS

    print("üöÄ D√âMARRAGE DE L'ANALYSE COMPL√àTE DES PERFORMANCES DE TRADING")
    print("=" * 80)
    print(f"üìÇ Direction analys√©e: {DIRECTION}")
    print(f"üìÅ R√©pertoire: {DIR}")
    print(f"‚öôÔ∏è  P√©riodes slope: {SLOPE_PERIODS}")  # Nouveau param√®tre affich√©
    print()

    # ====== CHARGEMENT DES DONN√âES ======
    print("üì• CHARGEMENT DES FICHIERS")
    print("=" * 30)

    try:
        TRAIN_COMPLETE, TRAIN_FILTERED, train_sessions = load_csv_complete(CSV_TRAIN)
        TEST_COMPLETE, TEST_FILTERED, test_sessions = load_csv_complete(CSV_TEST)
        VAL_COMPLETE, VAL_FILTERED, val_sessions = load_csv_complete(CSV_VAL)
        VAL1_COMPLETE, VAL1_FILTERED, val1_sessions = load_csv_complete(CSV_VAL1)
        UNSEEN_COMPLETE, UNSEEN_FILTERED, unseen_sessions = load_csv_complete(CSV_UNSEEN)
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des donn√©es: {e}")
        return

    # ====== NOUVELLE ANALYSE: WINRATES BRUTS ======
    filtered_datasets = [
        (TRAIN_FILTERED, "TRAIN"),
        (TEST_FILTERED, "TEST"),
        (VAL_FILTERED, "VALIDATION"),
        (VAL1_FILTERED, "VALIDATION 1"),
        (UNSEEN_FILTERED, "UNSEEN")
    ]

    # Analyse des winrates
    winrate_stats = {}
    for df, name in filtered_datasets:
        result = analyze_winrate_performance(df, name)
        if result:
            winrate_stats[name] = result

    # Affichage des analyses de winrate
    print_winrate_analysis(winrate_stats)
    print_comparative_winrate_analysis(winrate_stats)

    # ====== NOUVELLE ANALYSE: PATTERNS TEMPORELS ======
    temporal_patterns = {}
    for df, name in filtered_datasets:
        patterns = analyze_temporal_patterns(df, name)
        if patterns:
            temporal_patterns[name] = patterns

    print_temporal_analysis(temporal_patterns)

    # ====== NOUVELLE ANALYSE: CONDITIONS DE MARCH√â ======
    market_conditions = {}
    for df, name in filtered_datasets:
        conditions = analyze_market_conditions(df, name)
        if conditions:
            market_conditions[name] = conditions

    print_market_conditions_analysis(market_conditions)

    # ====== ANALYSE DES STATISTIQUES DE BOUGIES (ANCIEN CODE) ======
    print("\n" + "=" * 60)
    print("üîç ANALYSE DES STATISTIQUES DE BOUGIES")
    print("=" * 60)

    datasets = [
        (TRAIN_COMPLETE, "TRAIN"),
        (TEST_COMPLETE, "TEST"),
        (VAL_COMPLETE, "VALIDATION"),
        (VAL1_COMPLETE, "VALIDATION 1"),
        (UNSEEN_COMPLETE, "UNSEEN")
    ]

    all_stats = {}

    for df, name in datasets:
        stats = calculate_candle_stats(df, name)
        if stats:
            all_stats[name] = stats

    # ====== R√âSUM√â GLOBAL ======
    if all_stats:
        print("\n" + "=" * 60)
        print("üìã R√âSUM√â GLOBAL")
        print("=" * 60)

        # Calcul des moyennes pond√©r√©es
        total_candles = sum(stats['count'] for stats in all_stats.values())

        # Dur√©es
        stats_with_duration = {name: stats for name, stats in all_stats.items() if 'duration_mean_seconds' in stats}
        if stats_with_duration:
            weighted_avg_duration = sum(stats['duration_mean_seconds'] * stats['duration_count']
                                        for stats in stats_with_duration.values()) / sum(
                stats['duration_count'] for stats in stats_with_duration.values())

            print("üïê DUR√âES DES BOUGIES:")
            for name, stats in stats_with_duration.items():
                print(
                    f"{name:12}: {stats['duration_mean_seconds']:6.2f}s ({stats['duration_mean_minutes']:5.2f} min) - {stats['duration_count']:,} bougies")
            print(f"üéØ MOYENNE POND√âR√âE: {weighted_avg_duration:.2f}s ({weighted_avg_duration / 60:.2f} min)")

        # Volumes
        stats_with_volume = {name: stats for name, stats in all_stats.items() if 'volume_mean' in stats}
        if stats_with_volume:
            weighted_avg_volume = sum(stats['volume_mean'] * stats['volume_count']
                                      for stats in stats_with_volume.values()) / sum(
                stats['volume_count'] for stats in stats_with_volume.values())

            print(f"\nüìà VOLUME PAR TICK:")
            for name, stats in stats_with_volume.items():
                print(f"{name:12}: {stats['volume_mean']:8.2f} - {stats['volume_count']:,} bougies")
            print(f"üéØ MOYENNE POND√âR√âE: {weighted_avg_volume:.2f}")

        print(f"\nüìä TOTAL BOUGIES: {total_candles:,}")

        # ====== ANALYSE COMPARATIVE ======
        if len(all_stats) > 1:
            print("\n" + "=" * 60)
            print("üîç ANALYSE COMPARATIVE")
            print("=" * 60)

            # Comparaison dur√©es
            if stats_with_duration and len(stats_with_duration) > 1:
                print("üïê DUR√âES:")
                duration_means = [stats['duration_mean_seconds'] for stats in stats_with_duration.values()]
                duration_names = list(stats_with_duration.keys())

                min_duration = min(duration_means)
                max_duration = max(duration_means)
                min_idx = duration_means.index(min_duration)
                max_idx = duration_means.index(max_duration)

                print(f"‚ö° Plus rapide: {duration_names[min_idx]} ({min_duration:.2f}s)")
                print(f"üêå Plus lent: {duration_names[max_idx]} ({max_duration:.2f}s)")
                print(f"üìè √âcart: {max_duration - min_duration:.2f}s ({(max_duration - min_duration) / 60:.2f} min)")

                print(f"\nüìä VARIABILIT√â DUR√âES (Coefficient de variation):")
                for name, stats in stats_with_duration.items():
                    cv = (stats['duration_std_seconds'] / stats['duration_mean_seconds']) * 100
                    print(f"{name:12}: {cv:.1f}%")

            # Comparaison volumes
            if stats_with_volume and len(stats_with_volume) > 1:
                print(f"\nüìà VOLUMES:")
                volume_means = [stats['volume_mean'] for stats in stats_with_volume.values()]
                volume_names = list(stats_with_volume.keys())

                min_volume = min(volume_means)
                max_volume = max(volume_means)
                min_idx = volume_means.index(min_volume)
                max_idx = volume_means.index(max_volume)

                print(f"üìâ Plus faible: {volume_names[min_idx]} ({min_volume:.2f})")
                print(f"üìà Plus √©lev√©: {volume_names[max_idx]} ({max_volume:.2f})")
                print(f"üìè √âcart: {max_volume - min_volume:.2f}")

                print(f"\nüìä VARIABILIT√â VOLUMES (Coefficient de variation):")
                for name, stats in stats_with_volume.items():
                    cv = (stats['volume_std'] / stats['volume_mean']) * 100
                    print(f"{name:12}: {cv:.1f}%")

    else:
        print("‚ùå Aucune donn√©e valide trouv√©e dans les fichiers")

    # ====== V√âRIFICATION TEMPORELLE ======
    check_temporal_consistency(datasets)

    # ====== DIAGNOSTIC COMPLET TEST vs TOUS ======
    # ====== ANALYSE DES CONDITIONS PR√â-TRADE ======
    datasets_dict = {
        'TRAIN': (TRAIN_COMPLETE, TRAIN_FILTERED),
        'TEST': (TEST_COMPLETE, TEST_FILTERED),
        'VALIDATION': (VAL_COMPLETE, VAL_FILTERED),
        'VALIDATION 1': (VAL1_COMPLETE, VAL1_FILTERED),
        'UNSEEN': (UNSEEN_COMPLETE, UNSEEN_FILTERED)
    }

    # Analyser les 10 bougies pr√©c√©dant chaque trade
    pre_trade_results = add_ultra_fast_pre_trade_analysis_DISPLAY(datasets_dict, n_candles_before=SLOPE_PERIODS)
    slopes_analysis = add_missing_slopes_analysis(pre_trade_results)
    # Analyse compl√®te des patterns Spring/Momentum/√âpuisement
    pattern_analysis = add_pattern_analysis_to_main(pre_trade_results)
    dataset_names = ['TRAIN', 'TEST', 'VALIDATION', 'VALIDATION 1', 'UNSEEN']
    results = analyze_market_regimes(datasets_dict, dataset_names)

    # ====== RECOMMANDATIONS FINALES CONSOLID√âES ======
    print("\n" + "=" * 80)
    print("üí° RECOMMANDATIONS CONSOLID√âES POUR AM√âLIORER TEST")
    print("=" * 80)

    # Analyser les points faibles du dataset TEST
    if 'TEST' in winrate_stats and winrate_stats['TEST'][0] is not None:
        test_stats = winrate_stats['TEST'][0]

        print("üîç DIAGNOSTIC FINAL TEST:")
        print(f"‚Ä¢ Winrate: {test_stats['winrate']:.2f}% (Position: 5/5)")
        print(f"‚Ä¢ Consistance: {test_stats['winrate_per_session_std']:.2f}% d'√©cart-type")
        print(f"‚Ä¢ Sessions profitables: {test_stats['profitable_sessions_pct']:.1f}%")
        if results['quality_score']:
            print(f"‚Ä¢ Score qualit√© conditions: {results['quality_score']:.0f}%")
        print(f"‚Ä¢ VPT divergents d√©tect√©s: {len(results['divergent_signals'])}")

        print("\nüéØ PLAN D'ACTION PRIORITAIRE:")

        # Consolidation des recommandations
        all_recommendations = []

        # De l'analyse winrate
        if test_stats['winrate_per_session_std'] > 10:
            all_recommendations.append("üìä CONSISTANCE: Forte variabilit√© entre sessions")

        # De l'analyse des r√©gimes de march√©
        if results['quality_score'] and results['quality_score'] < 50:
            all_recommendations.append("üå™Ô∏è CONDITIONS: R√©gime de march√© difficile")

        if len(results['divergent_signals']) > 3:
            all_recommendations.append("üìà VPT: Signaux instables temporellement")

        # Actions concr√®tes
        print("1. ANALYSE D√âTAILL√âE DES SESSIONS PROBL√âMATIQUES")
        print("   ‚Üí Identifier les 10 pires sessions de TEST")
        print("   ‚Üí Analyser leurs conditions de march√© communes")

        print("2. RECALIBRAGE DES INDICATEURS VPT")
        print(f"   ‚Üí Re-tester les {len(results['divergent_signals'])} VPT divergents")
        print("   ‚Üí Impl√©menter des seuils adaptatifs par r√©gime")

        print("3. FILTRAGE TEMPOREL INTELLIGENT")
        print("   ‚Üí √âviter les heures/jours les moins performants")
        print("   ‚Üí Concentrer sur les cr√©neaux optimaux")

        print("4. ADAPTATION AUX CONDITIONS DE MARCH√â")
        print("   ‚Üí Ajuster stops/TP selon la volatilit√©")
        print("   ‚Üí Filtrer les p√©riodes de micro-volatilit√©")

    print(f"\nüî¨ SURVEILLANCE CONTINUE:")
    print(f"   ‚Ä¢ Stabilit√© VPT: {len(results['divergent_signals'])} indicateurs √† surveiller")
    print(f"   ‚Ä¢ Qualit√© conditions: Maintenir >60%")
    print(f"   ‚Ä¢ Consistance sessions: Viser <8% d'√©cart-type")
    print(f"   ‚Ä¢ Winrate global: Objectif >52%")

    print("\n" + "=" * 80)
    print("‚úÖ ANALYSE COMPL√àTE TERMIN√âE")
    print("=" * 80)
    # Apr√®s l'analyse pr√©-trade existante:
    pre_trade_results = add_ultra_fast_pre_trade_analysis_DISPLAY(datasets_dict, n_candles_before=SLOPE_PERIODS)
    slopes_analysis = add_missing_slopes_analysis(pre_trade_results)
    pattern_analysis = add_pattern_analysis_to_main(pre_trade_results)

    universal_results = test_universal_anti_spring_strategy_fixed(
        datasets_dict,
        pre_trade_results,
        label_col='class_binaire'
    )
if __name__ == "__main__":
    main()